{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMA 2022S 865, Individual Assignment 1\n",
    "\n",
    "Version 2: Updated January 1, 2022.\n",
    "\n",
    "- [Shikhar, Goyal]\n",
    "- [20244860]\n",
    "- [Section 1]\n",
    "- [1-March-2022]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 - ELI5\n",
    "\n",
    "_“If you can't explain it simply, you don't understand it well enough.” – Albert Einstein_\n",
    "\n",
    "Explaining technical concepts to a non-technical audience is an underappreciated skill; one which the GMMA/MMA program aims to give its students; and one that will truly set you apart in the job market. The only way to gain a skill is by practice, so here we go.\n",
    "\n",
    "Answer each question below as though you were talking to a 5 year old (equivalently: a grandma, or a completely non-technical manager, or an Ivey grad). Use your own words. Use analogies where possible. Examples are better than theory. Keep it short, but be complete. Use simple, plain English. Do not use business buzzwords like _actualize, empower, fungible, leverage, or synergize_. Do not use technical buzzwords that most people don’t know like _model, agile, bandwidth, IoT, blockchain, AR, VR, actionable insights_. Inform the audience without going into too much technical detail. Your goal is to truly help them understand, not to give what you feel is a “technically precise” answer and move on (but they still don’t understand!). Don’t be that guy!\n",
    "\n",
    "Please keep each answer to 1000 characters or less.\n",
    "\n",
    "Finally, feel free to use [Markdown syntax](https://www.markdownguide.org/basic-syntax/) to format your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: What is “Big Data” and how is it different than “regular data”?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first understand what is 'Data'. 'Data' is any information that is generated, transmitted, received, stored and processed. The information can be passed on as an audio, a video, a letter or a table that can be stored and processed. For example, a list of grocery to be purchased is a data. You can call the grocer and tell them this list, You can record a video of you reading the list or you can send the list on a paper,either way it is an information that grocer needs to store and work on.\n",
    "\n",
    "Now 'Big' refers to the size and speed with which that data is generated. Think like you send a small piece of paper to the near-by grocer with the list of items you want at the beginning of every month. It is easy for the grocer to keep that paper in their drawer and work on it and deliver your stuff in time. But now you make it \"big\",everyday, you send a 500 page list, along with your recorded video reading that list and then you also call the grocer for the stuff you want to be delivered. Here you have done three things, you have send the data in different forms, you have increased the amount of data and you have also increased the frequency with which you are sending the information. Making it challenging for your grocer to prioritise and work on your list. Similarly, organizations are receiving huge amount of data from different sources and different formats, and this data is called 'Big Data'. In terminology terms, 'Big Data' is the data with huge volume, high velocity and different veracity.\n",
    "\n",
    "Now lets understand the difference between Big Data and Regular Data. Big Data as mentioned above are big in size, fast in speed and of different variety. While regular data is structured, easy to process and store. Lets understand the difference with an example.\n",
    "\n",
    "10 years back our phones were not as smart as they are now. We all used a basic phone which allowed us to make a call and send/receive only a text message. Our inbox consisted of text messages from our family and friends, those text messages were sorted by date and the name of the sender. It was easy to retrieve any historical message and the size of the messages were also very small. Very structured and very easy to use. In the same way, organization generate regular transaction data that are structured into tables and are easy to retrieve and takes less storage. This is called 'Regular Data'.\n",
    "\n",
    "10 years fast forward, today we all have smartphones where we are using applications like 'Whatsapp'. Through whatsapp we can stay connected to everyone around the world. We continuously send/receive text messages, audio recordings, videos, documents, images,contacts etc. all types of data on this application. The size of the data that we share can go into GBs. All this information that we are getting on whatsapp is either getting stored in phone memory, memory card or in cloud without any particular order and with a lot of junk as well. If we are to retrieve a photograph of ours from years back, then it will take a lot more effort to find that picture. This data is unstuctured, huge in its size, the magnitude with which we are getting and sharing data is high, it comes in different format with lot more information and requires a lot more effort. This is called 'Big Data'. Companies are having a lot of big data at their disposal but to find relevant information from that data is the biggest challenge nowadays. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: What is Hadoop? Hint: What problems in previous data storage and processing was Hadoop designed to solve? How did Hadoop accomplish that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop solved many problems that were being faced for storage and processing. as the data size grew, the organizations began scaling up their computers by adding more storage capacity and processing power. But it was not a desired solution as the chances of system failure were high,if anything goes wrong with the computer the whole server will break. Additionally, two computers were being used, one for storage and other for processing, therefore transferring data to the computers in such cases became a huge bottle neck.\n",
    "\n",
    "Lets understand the problems solved by Hadoop ecosystem and the way it did it by the following example:\n",
    "\n",
    "###### Old way of working:\n",
    "\n",
    "You have a grocery shop and you receive orders from your customers that you need to deliver on time. Life is easy for you when you are receiving small number of orders from customers, you open your store 8 hours a day and you are able to work and close all orders on daily basis. Eventually people got to know about you and you started receiving 100 times more orders. You took the entire load on yourself and you had to work 12 hours a day making sure that every customer is getting the item they ordered from your store. Not only are you compiling the orders but you have to store the order history in a proper format.It is hard for you to keep track of all the orders and sometimes you make mistake out of confusion.On the days you are ill, you are not able to work and none of your customer's order gets worked on.\n",
    "\n",
    "So you now decided to change the way you are working and employ \"Hadoop\" methodology.\n",
    "\n",
    "###### New way of working:\n",
    "\n",
    "You hired 5 employees in your store. Now when you get orders from your customer, you assign that order to one of your employee. As a backup, you make a copy of that order and give it to another employee just incase the earlier one losses it. This way you divide the orders and delegate work amongst your employees and once all the orders are ready for delivery, they are brought at one place and are sent out for delivery. By using Hadoop methodology you have mitigated the risk of failing to deliver, if one employee call in sick on any other day, the other employee can work on the list and get the items delivered.\n",
    "\n",
    "This is the same way Hadoop was able to solve previous data storage and processing issues. Instead of one computer storing all the data and doing the processing, Hadoop distributed the storage and processing task to different virtual computers. When a data is to be stored, Hadoop divided the storage in different computers so that noone computer has to take the entire load. Similarly, when that data is required to be processed, hadoop distributed the processing task amongst these virtual computers, which after completing their respective task gave the required output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: How does Big Data and the cloud help Machine Learning? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning has really benefitted due to Big Data and the Cloud technology. Big Data discipline has made more and varied data available for the Machine Learning models to train on, while Cloud Technology has made the storage and processing of this big data possible for Machine Learning consumption. The more the data on which Machine Learning models are trained, the better will be the performance of the model, thus more the impact. Cloud Technology has made the ecosystems like Hadoop to thrive and enable organizations to store and process large amount of data through remote computers without worrying about the infrastructure.\n",
    "\n",
    "Lets understand this using a more general example:\n",
    "\n",
    "Suppose you (Machine Learning Model) want to learn to play piano, your friends got to know about your interest, and they began sending you hundreds of piano videos on your Whatsapp (Messaging App). You watch all those videos and ask for more videos from your friends. But because your phone has a limited storage, your friends cannot send any more videos. You have two options, either you purchase a new phone and store the remaining videos in that phone, which will also mean cost of maintaining two phones or you can avail \"Cloud\" technology. In simple terms, what cloud technology will do, whenever you receive a video on Whatsapp, it will store that video in any of the mobile device in the world with full ability of yours to retrieve that video at any point of time. So now you have don’t have any limit on the number of videos you can receive and store,  also no need for you to buy any extra phone for yourself.\n",
    "Now you have watched all the videos on how to play piano as a result you know a lot more new ways of playing the instrument. This has resulted in your being better trained at playing piano and outperform your peers in any concert or competition.\n",
    "Do you see, with so much available information you can do so much, just imagine how much Machine Learning models can do with so much information!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: What is NoSQL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NoSQL are the non-relationnal databases which store data not in conventional table format instead it stores the data that best suits the type of data being stored to ensure faster querying and scalability.\n",
    "\n",
    "Lets understand this by taking a simple example that will losely make you understand what NoSQL does:\n",
    "\n",
    "When you put your clothes in your wardrobe, how do you put it? You fold it and then you stack it inside the wardrobe. And you do it the same way for all the clothes, no matter the color or the size of the cloth. This is the same way SQL stores relational data, it creates a table for all the data it gets and stores it, so as to use it later.\n",
    "\n",
    "But now what about stuff that you keep in your \"Store room\" (Furnace room, Storage). In the store room of your house you keep different things of different size, shape and form. You keep empty suitcases, you keep extra utensils, you keep plastic bags etc. in the store room. And to make sure that you are able to easily get stuff out of the store room, you organize it based on the type of material you are keeping. You keep heavy stuff at the bottom, you keep lighter stuff at the top, fragile items like glass are wrapped and kept in a separate slot so that it does not break, you get my point here. This is what NoSQL also do, it does not store every data as a table in the non-relational databases. Instead,it stores the data in the format that best suits the type of that data being stored. \n",
    "\n",
    "NoSQL datastores are:\n",
    "\n",
    "###### 1. Key Value Store: \n",
    "\n",
    "In this format the data is stored in a Key-Value pair. The database assigns unique key to each value and then later fetches that value by referring to that unique key. Just the way, someone would organize and store their house keys. They would assign specific tag to each key. Car Key, Bedroom Key, Storeroom Key etc. and when they want a key for a specific purpose, they would simply refer to the tag and fetch the required key.\n",
    "\n",
    "This type of format is ideal for large data objects like audios, images etc. storage\n",
    "\n",
    "###### 2. Document Database Store\n",
    "\n",
    "Document Database stores all the information of an entity in a single instance and assigns it a key to refer to while fetching the data.\n",
    "\n",
    "It is like the way we keep the name, address, contact information of our family and friends in our diary. One page has the name, address and number of one friend and other page has the information for the other friend. Everything is indexed by alphabet or sticky notes.\n",
    "\n",
    "###### 3. Graph Store\n",
    "\n",
    "Graph store uses graph structure to store, query and manage network and connections in the dataset.\n",
    "\n",
    "Nowadays, social media sites store our data as a 'Graph store' so that they can recommend us as to who we might know. Graph store is good to determine relationship between people or nodes.\n",
    "\n",
    "###### 4. Columnar Data Store\n",
    "\n",
    "These are like the relationnal databases but to improve the performance of data retrieval and data processing, the data is stored in column instead of rows. \n",
    "\n",
    "So in relationnal databases, each row has the name of a customer, their age, their profession information for one customer. In the columnar data store, the same information is stored as a column, that is to say each column will have all the information of a customer. And the number of columns is equal to the number of customers in the database.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Name three ways topic modeling could help a bank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Modelling is an unsupervised machine learning technology that derives the central idea of the document by classifying the documents under different topics.\n",
    "As banks receive a lot of documents on daily basis in the form complaint emails, newspaper articles and tweets, they can really leverage this technology in their benefit.\n",
    "Three ways which I believe topic modelling can really help the banks are:\n",
    "\n",
    "###### 1.\tClassifying the main topic of the ticket raised by the customer and routing the complaint to the concerned department\n",
    "Bank’s customer service and customer experience defines the quality of its operations. As per torontosun.com (https://torontosun.com/news/national/banks-receive-5-8-million-complaints-a-year-averaging-207-days-for-resolution-report) banks receive 5.8 million complaints in a year and it takes an average of 207 days for the resolution. Though there are multiple level of complexities involved with customer service, but I believe Topic Modelling can help improving this process to some extent.\n",
    "All the complaint tickets raised by the customers can be fed to a Topic Model, the unsupervised machine learning algorithm will automatically discover the topics in tickets, it will give a weightage of the occurrence of particular type of key word in each topic. Data Scientist can look at the top 20 or 30 weighted keywords, and that will be helpful to understand the central of the document. For example, if a document has top 5 high weighted keywords as [Credit, Card, payment, account, bank, failed], then we can certainly assume that this ticket meant for Credit card department and can be automatically routed to the concerned department without any human intervention and thus speeding the process.\n",
    "\n",
    "###### 2.\tBanks can identify the news that focus on financial market and industry that can influence their performance in the market.\n",
    "\n",
    "Stocks and Financial Market can be affected by macroeconomic conditions like company news and performance, industry performance, investor sentiment and economic factors. Banks can feed all the web news to the topic modelling algorithm and looking at the top weighted words, banks define the topic of discussion and can refer to the news that interest their strategy.\n",
    "\n",
    "For example, if on any given day there is a news regarding the potential growth of bitcoin investment and is flagged as the most important word in a topic, the banks can immediately flex their strategy to make the most of this growth in bitcoin. Similarly, topic modelling can help banks to keep a check on the news related to their competitors.\n",
    "\n",
    "###### 3.\tBanks can use Topic Modelling for internal employee surveys and improvement.\n",
    "A lot of improvement in an organization comes from within. Banks employee lot of talented people, for example Royal Bank of Canada employees 85,301 people (https://en.wikipedia.org/wiki/Royal_Bank_of_Canada). Banks can take regular feedbacks from its employees and feed those feedbacks to topic model algorithm.\n",
    "Banks will get to know the central topic of each document and can start working on that area of improvement. For example, if 10 employees submit their feedback and the algorithm classifies dataset into 2 topics with [‘culture’, ’Hierarchy’] being the top weighted words for one topic,then the HR team will get to know that feedbacks that belong to topic one and can start focussing on those feedbacks with culture is the prime focus.\n",
    "Topic Model is very useful to eliminate the manual labour that goes in reading the documents to get an idea of central theme of what is being said.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6: What is Apache Spark, exactly, and what are its pros and cons?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache spark is a framework for fast data processing on very large data sets. It can do this either by distributing data processing task across multiple computers, either on its own or by collaboration with other computing tools. There are multiple pros of this technology due to which it is one of the most popular framework in cloud computing space. The pros of Apache Spark are:\n",
    "\n",
    "##### Pros:\n",
    "1.\tIt is very fast in processing. Spark enables applications in Hadoop clusters to run in-memory at up to 100x faster than MapReduce, while also delivering significant speed-ups when running purely on disk.\n",
    "\n",
    "2.\tSpark SQL provides an interface for users to query their data from Spark RDDs as well as other data sources such as Hive tables, parquet files and JSON files. Spark’s APIs in Python, Scala & Java make it easy to build parallel apps.\n",
    "\n",
    "3.\tSpark has a huge open source community that contributed to its development and user support.\n",
    "\n",
    "4.\tSpark performs “Lazy Evaluation”, that is to say, there has to be a trigger for enforcing any transformation in the dataset. It saves computation and increases the speed as well manageability.\n",
    "\n",
    "##### Cons:\n",
    "\n",
    "1.\tIt lacks the file management capabilities as a result Apache Spark relies on third party system for file management.\n",
    "\n",
    "2.\tCost of Apache Spark is very high because of the expensive in-memory data processing, that requires a lot of RAM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Sentiment Analysis via the ML-based approach\n",
    "\n",
    "Download the “Product Sentiment” dataset from the course portal: sentiment_train.csv and sentiment_test.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.a. Loading and Prep\n",
    "\n",
    "Load, clean, and preprocess the data as you find necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\ishik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2400 entries, 0 to 2399\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Sentence  2400 non-null   object\n",
      " 1   Polarity  2400 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 37.6+ KB\n",
      "None\n",
      "                                            Sentence  Polarity\n",
      "0                           Wow... Loved this place.         1\n",
      "1                                 Crust is not good.         0\n",
      "2          Not tasty and the texture was just nasty.         0\n",
      "3  Stopped by during the late May bank holiday of...         1\n",
      "4  The selection on the menu was great and so wer...         1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2400 entries, 0 to 2399\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Sentence  2400 non-null   object\n",
      " 1   Polarity  2400 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 37.6+ KB\n",
      "None\n",
      "                                            Sentence  Polarity\n",
      "0                           Wow... Loved this place.         1\n",
      "1                                 Crust is not good.         0\n",
      "2          Not tasty and the texture was just nasty.         0\n",
      "3  Stopped by during the late May bank holiday of...         1\n",
      "4  The selection on the menu was great and so wer...         1\n"
     ]
    }
   ],
   "source": [
    "#Importing all the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "nltk.download(\"all\")\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import string\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "##############LOAD DATA#################################\n",
    "df_train = pd.read_csv(\"sentiment_train.csv\")\n",
    "\n",
    "print(df_train.info())\n",
    "print(df_train.head())\n",
    "\n",
    "df_test = pd.read_csv(\"sentiment_test.csv\")\n",
    "\n",
    "print(df_train.info())\n",
    "print(df_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100    Our server was fantastic and when he found out...\n",
       "101    The only good thing was our waiter, he was ver...\n",
       "102    Best Buffet in town, for the price you cannot ...\n",
       "103    I LOVED their mussels cooked in this wine redu...\n",
       "104    This is one of the better buffets that I have ...\n",
       "                             ...                        \n",
       "195        The best place to go for a tasty bowl of Pho!\n",
       "196             The live music on Fridays totally blows.\n",
       "197    I've never been more insulted or felt disrespe...\n",
       "198                                 Very friendly staff.\n",
       "199                               It is worth the drive.\n",
       "Name: Sentence, Length: 100, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the reviews in the train dataset from 100 to 200 rows\n",
    "df_train[\"Sentence\"][100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Polarity', ylabel='count'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting count plot for checking the data imbalance\n",
    "sns.countplot(x = \"Polarity\", data = df_train, palette = [\"r\",\"g\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is no imbalancing in the data as the label are equally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>#NAME?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>#NAME?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>#NAME?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>#NAME?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence  Polarity\n",
       "26    #NAME?         0\n",
       "71    #NAME?         1\n",
       "219   #NAME?         1\n",
       "904   #NAME?         0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for missing data\n",
    "df_train[df_train[\"Sentence\"] ==\"#NAME?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting rows with error entries\n",
    "df_train.drop([26,71,219,904],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2396, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resetting the index numbers as that will affect the cout vectorization later\n",
    "df_train = df_train.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Sentence, Polarity]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the rows after removing them\n",
    "df_train[df_train[\"Sentence\"] ==\"#NAME?\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying sentiment analysis to each row of the column\n",
    "df_train[\"scores\"] = df_train[\"Sentence\"].apply(lambda p:sid.polarity_scores(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2396, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a \"compound score\" feature\n",
    "df_train[\"compound_score\"] = df_train[\"scores\"].apply(lambda di:di[\"compound\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>scores</th>\n",
       "      <th>compound_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.435, 'pos': 0.565, 'comp...</td>\n",
       "      <td>0.5994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "      <td>{'neg': 0.445, 'neu': 0.555, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.3412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "      <td>{'neg': 0.34, 'neu': 0.66, 'pos': 0.0, 'compou...</td>\n",
       "      <td>-0.5574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.093, 'neu': 0.585, 'pos': 0.322, 'co...</td>\n",
       "      <td>0.6908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.728, 'pos': 0.272, 'comp...</td>\n",
       "      <td>0.6249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Polarity  \\\n",
       "0                           Wow... Loved this place.         1   \n",
       "1                                 Crust is not good.         0   \n",
       "2          Not tasty and the texture was just nasty.         0   \n",
       "3  Stopped by during the late May bank holiday of...         1   \n",
       "4  The selection on the menu was great and so wer...         1   \n",
       "\n",
       "                                              scores  compound_score  \n",
       "0  {'neg': 0.0, 'neu': 0.435, 'pos': 0.565, 'comp...          0.5994  \n",
       "1  {'neg': 0.445, 'neu': 0.555, 'pos': 0.0, 'comp...         -0.3412  \n",
       "2  {'neg': 0.34, 'neu': 0.66, 'pos': 0.0, 'compou...         -0.5574  \n",
       "3  {'neg': 0.093, 'neu': 0.585, 'pos': 0.322, 'co...          0.6908  \n",
       "4  {'neg': 0.0, 'neu': 0.728, 'pos': 0.272, 'comp...          0.6249  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding negative score column to the dataframe\n",
    "df_train[\"negative_score\"] = df_train[\"scores\"].apply(lambda di:di[\"neg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding positive score column to the dataframe\n",
    "df_train[\"positive_score\"] = df_train[\"scores\"].apply(lambda di:di[\"pos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating X and Y train dataset\n",
    "X = df_train.drop('Polarity',axis = 1,inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2396, 5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train['Polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2396,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Removing Punctuations\n",
    "2. Removing Numbers from the reviews\n",
    "3. Lower-casing the sentences\n",
    "4. Splitting the words by white spaces\n",
    "5. Removing \"StopWords\" from the sentences\n",
    "6. Lemmatization of the words\n",
    "7. Joining the words back to form a sentence\n",
    "8. Count Vectorization\n",
    "9. Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing the entire data by defining the function called 'text_clean'\n",
    "\n",
    "def text_clean(text):\n",
    "    r = text.translate(str.maketrans(\"\",\"\",string.punctuation)) #Remove All Punctuations\n",
    "    r = re.sub(r'\\d+', '',r) #remove numbers\n",
    "    r = r.lower() #Lowercase the string\n",
    "    r = r.split() #Split the strings on white spaces\n",
    "    r = [word for word in r if word not in stopwords.words('english')] #Remove Stopwords\n",
    "    r = [lemmatizer.lemmatize(word) for word in r] #Lemmatization\n",
    "    r = ' '.join(r) #Joining back the words to make the entire string\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new feature with processed text\n",
    "X['processedText'] = X['Sentence'].apply(lambda x: text_clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>scores</th>\n",
       "      <th>compound_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>processedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.435, 'pos': 0.565, 'comp...</td>\n",
       "      <td>0.5994</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.565</td>\n",
       "      <td>wow loved place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>{'neg': 0.445, 'neu': 0.555, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.3412</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.000</td>\n",
       "      <td>crust good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>{'neg': 0.34, 'neu': 0.66, 'pos': 0.0, 'compou...</td>\n",
       "      <td>-0.5574</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.000</td>\n",
       "      <td>tasty texture nasty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>{'neg': 0.093, 'neu': 0.585, 'pos': 0.322, 'co...</td>\n",
       "      <td>0.6908</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.322</td>\n",
       "      <td>stopped late may bank holiday rick steve recom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.728, 'pos': 0.272, 'comp...</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.272</td>\n",
       "      <td>selection menu great price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Now I am getting angry and I want my damn pho.</td>\n",
       "      <td>{'neg': 0.451, 'neu': 0.451, 'pos': 0.098, 'co...</td>\n",
       "      <td>-0.6908</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.098</td>\n",
       "      <td>getting angry want damn pho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Honeslty it didn't taste THAT fresh.)</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>honeslty didnt taste fresh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The potatoes were like rubber and you could te...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.802, 'pos': 0.198, 'comp...</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.198</td>\n",
       "      <td>potato like rubber could tell made ahead time ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The fries were great too.</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.494, 'pos': 0.506, 'comp...</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.506</td>\n",
       "      <td>fry great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A great touch.</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.196, 'pos': 0.804, 'comp...</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.804</td>\n",
       "      <td>great touch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0                           Wow... Loved this place.   \n",
       "1                                 Crust is not good.   \n",
       "2          Not tasty and the texture was just nasty.   \n",
       "3  Stopped by during the late May bank holiday of...   \n",
       "4  The selection on the menu was great and so wer...   \n",
       "5     Now I am getting angry and I want my damn pho.   \n",
       "6              Honeslty it didn't taste THAT fresh.)   \n",
       "7  The potatoes were like rubber and you could te...   \n",
       "8                          The fries were great too.   \n",
       "9                                     A great touch.   \n",
       "\n",
       "                                              scores  compound_score  \\\n",
       "0  {'neg': 0.0, 'neu': 0.435, 'pos': 0.565, 'comp...          0.5994   \n",
       "1  {'neg': 0.445, 'neu': 0.555, 'pos': 0.0, 'comp...         -0.3412   \n",
       "2  {'neg': 0.34, 'neu': 0.66, 'pos': 0.0, 'compou...         -0.5574   \n",
       "3  {'neg': 0.093, 'neu': 0.585, 'pos': 0.322, 'co...          0.6908   \n",
       "4  {'neg': 0.0, 'neu': 0.728, 'pos': 0.272, 'comp...          0.6249   \n",
       "5  {'neg': 0.451, 'neu': 0.451, 'pos': 0.098, 'co...         -0.6908   \n",
       "6  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...          0.0000   \n",
       "7  {'neg': 0.0, 'neu': 0.802, 'pos': 0.198, 'comp...          0.5719   \n",
       "8  {'neg': 0.0, 'neu': 0.494, 'pos': 0.506, 'comp...          0.6249   \n",
       "9  {'neg': 0.0, 'neu': 0.196, 'pos': 0.804, 'comp...          0.6249   \n",
       "\n",
       "   negative_score  positive_score  \\\n",
       "0           0.000           0.565   \n",
       "1           0.445           0.000   \n",
       "2           0.340           0.000   \n",
       "3           0.093           0.322   \n",
       "4           0.000           0.272   \n",
       "5           0.451           0.098   \n",
       "6           0.000           0.000   \n",
       "7           0.000           0.198   \n",
       "8           0.000           0.506   \n",
       "9           0.000           0.804   \n",
       "\n",
       "                                       processedText  \n",
       "0                                    wow loved place  \n",
       "1                                         crust good  \n",
       "2                                tasty texture nasty  \n",
       "3  stopped late may bank holiday rick steve recom...  \n",
       "4                         selection menu great price  \n",
       "5                        getting angry want damn pho  \n",
       "6                         honeslty didnt taste fresh  \n",
       "7  potato like rubber could tell made ahead time ...  \n",
       "8                                          fry great  \n",
       "9                                        great touch  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_text variable containing the list of sentence column\n",
    "X_text = X[\"processedText\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2396,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CountVectorization to create Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv =CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying countvectorizer to X_text column\n",
    "count_vect = cv.fit_transform(X_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2396x3628 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 13321 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While reading through the reviews under the \"Sentence\" column, it was observed that reviews were not for one specific topic or business, rather it was a mixed topic. To dig deeper, I performed Topic Modelling and created another feature with topic labels. Though it did not improve the score of the model (nor decreased) but it proved to be a good effort for my own learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required library for performing Topic Modelling\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of components set to 4\n",
    "LDA = LatentDirichletAllocation(n_components = 4,random_state = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=4, random_state=16)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA.fit(count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 15 words for topic #0\n",
      "['price', 'go', 'like', 'ive', 'time', 'really', 'work', 'ever', 'one', 'phone', 'service', 'back', 'good', 'food', 'great']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The top 15 words for topic #1\n",
      "['terrible', 'one', 'well', 'go', 'time', 'like', 'excellent', 'movie', 'case', 'product', 'service', 'recommend', 'good', 'would', 'place']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The top 15 words for topic #2\n",
      "['car', 'film', 'time', 'problem', 'make', 'waste', 'movie', 'think', 'work', 'pretty', 'good', 'place', 'use', 'dont', 'phone']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The top 15 words for topic #3\n",
      "['right', 'poor', 'way', 'work', 'comfortable', 'nice', 'ear', 'good', 'bad', 'love', 'phone', 'headset', 'battery', 'sound', 'quality']\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Grab the highest probability words for each topic\n",
    "for i,topic in enumerate(LDA.components_):\n",
    "    print(f\"The top 15 words for topic #{i}\")\n",
    "    print([cv.get_feature_names()[index] for index in topic.argsort()[-15:]])\n",
    "    print('\\n')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four topics are created as we provided number of components to be 4. When looking at the high probability words we can name the topics:\n",
    "\n",
    "1. Topic 0 = Talks something about food and phone and their prices\n",
    "2. Topic 1 = Talks something about the type of food or movie, mostly something not good.\n",
    "3. Topic 2 = Mixed topic about the good quality of food or phone\n",
    "4. Topic 3 = Talks about the goodness or badness of the environment as well as the sound quality of phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming the count_vectorized array so as to create a column for topics\n",
    "topic_results = LDA.transform(count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a column which will have the topic value of the highest probability keywords\n",
    "X[\"topicmodel\"] = topic_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>scores</th>\n",
       "      <th>compound_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>processedText</th>\n",
       "      <th>topicmodel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.435, 'pos': 0.565, 'comp...</td>\n",
       "      <td>0.5994</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.565</td>\n",
       "      <td>wow loved place</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>{'neg': 0.445, 'neu': 0.555, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.3412</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.000</td>\n",
       "      <td>crust good</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>{'neg': 0.34, 'neu': 0.66, 'pos': 0.0, 'compou...</td>\n",
       "      <td>-0.5574</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.000</td>\n",
       "      <td>tasty texture nasty</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>{'neg': 0.093, 'neu': 0.585, 'pos': 0.322, 'co...</td>\n",
       "      <td>0.6908</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.322</td>\n",
       "      <td>stopped late may bank holiday rick steve recom...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.728, 'pos': 0.272, 'comp...</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.272</td>\n",
       "      <td>selection menu great price</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Now I am getting angry and I want my damn pho.</td>\n",
       "      <td>{'neg': 0.451, 'neu': 0.451, 'pos': 0.098, 'co...</td>\n",
       "      <td>-0.6908</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.098</td>\n",
       "      <td>getting angry want damn pho</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Honeslty it didn't taste THAT fresh.)</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>honeslty didnt taste fresh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The potatoes were like rubber and you could te...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.802, 'pos': 0.198, 'comp...</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.198</td>\n",
       "      <td>potato like rubber could tell made ahead time ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The fries were great too.</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.494, 'pos': 0.506, 'comp...</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.506</td>\n",
       "      <td>fry great</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A great touch.</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.196, 'pos': 0.804, 'comp...</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.804</td>\n",
       "      <td>great touch</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Service was very prompt.</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>service prompt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Would not go back.</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>would go back</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The cashier had no care what so ever on what I...</td>\n",
       "      <td>{'neg': 0.094, 'neu': 0.769, 'pos': 0.137, 'co...</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.137</td>\n",
       "      <td>cashier care ever say still ended wayyy overpr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I tried the Cape Cod ravoli, chicken,with cran...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>tried cape cod ravoli chickenwith cranberrymmmm</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I was disgusted because I was pretty sure that...</td>\n",
       "      <td>{'neg': 0.214, 'neu': 0.44, 'pos': 0.346, 'com...</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.346</td>\n",
       "      <td>disgusted pretty sure human hair</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I was shocked because no signs indicate cash o...</td>\n",
       "      <td>{'neg': 0.429, 'neu': 0.571, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.5423</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.000</td>\n",
       "      <td>shocked sign indicate cash</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Highly recommended.</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.323, 'pos': 0.677, 'comp...</td>\n",
       "      <td>0.2716</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.677</td>\n",
       "      <td>highly recommended</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Waitress was a little slow in service.</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>waitress little slow service</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>This place is not worth your time, let alone V...</td>\n",
       "      <td>{'neg': 0.314, 'neu': 0.686, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.3952</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.000</td>\n",
       "      <td>place worth time let alone vega</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>did not like at all.</td>\n",
       "      <td>{'neg': 0.345, 'neu': 0.655, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.2755</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.000</td>\n",
       "      <td>like</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The Burrittos Blah!</td>\n",
       "      <td>{'neg': 0.458, 'neu': 0.542, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.1759</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.000</td>\n",
       "      <td>burrittos blah</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>The food, amazing.</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.345, 'pos': 0.655, 'comp...</td>\n",
       "      <td>0.5859</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.655</td>\n",
       "      <td>food amazing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Service is also cute.</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.5, 'pos': 0.5, 'compound...</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>service also cute</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>I could care less... The interior is just beau...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.458, 'pos': 0.542, 'comp...</td>\n",
       "      <td>0.7964</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.542</td>\n",
       "      <td>could care le interior beautiful</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>So they performed.</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>performed</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>That's right....the red velvet cake.....ohhh t...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.706, 'pos': 0.294, 'comp...</td>\n",
       "      <td>0.5777</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.294</td>\n",
       "      <td>thats rightthe red velvet cakeohhh stuff good</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>This hole in the wall has great Mexican street...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.601, 'pos': 0.399, 'comp...</td>\n",
       "      <td>0.8074</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.399</td>\n",
       "      <td>hole wall great mexican street taco friendly s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Took an hour to get our food only 4 tables in ...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.796, 'pos': 0.204, 'comp...</td>\n",
       "      <td>0.5984</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.204</td>\n",
       "      <td>took hour get food table restaurant food luke ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>The worst was the salmon sashimi.</td>\n",
       "      <td>{'neg': 0.451, 'neu': 0.549, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.6249</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.000</td>\n",
       "      <td>worst salmon sashimi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Also there are combos like a burger, fries, an...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.848, 'pos': 0.152, 'comp...</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.152</td>\n",
       "      <td>also combo like burger fry beer decent deal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>This was like the final blow!</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.642, 'pos': 0.358, 'comp...</td>\n",
       "      <td>0.4199</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.358</td>\n",
       "      <td>like final blow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>I found this place by accident and I could not...</td>\n",
       "      <td>{'neg': 0.423, 'neu': 0.577, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.7074</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.000</td>\n",
       "      <td>found place accident could happier</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>seems like a good quick place to grab a bite o...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.701, 'pos': 0.299, 'comp...</td>\n",
       "      <td>0.7391</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.299</td>\n",
       "      <td>seems like good quick place grab bite familiar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Overall, I like this place a lot.</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.615, 'pos': 0.385, 'comp...</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.385</td>\n",
       "      <td>overall like place lot</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>The only redeeming quality of the restaurant w...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>redeeming quality restaurant inexpensive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Ample portions and good prices.</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.58, 'pos': 0.42, 'compou...</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.420</td>\n",
       "      <td>ample portion good price</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Poor service, the waiter made me feel like I w...</td>\n",
       "      <td>{'neg': 0.283, 'neu': 0.609, 'pos': 0.109, 'co...</td>\n",
       "      <td>-0.6124</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.109</td>\n",
       "      <td>poor service waiter made feel like stupid ever...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>My first visit to Hiro was a delight!</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.589, 'pos': 0.411, 'comp...</td>\n",
       "      <td>0.6360</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.411</td>\n",
       "      <td>first visit hiro delight</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Service sucks.</td>\n",
       "      <td>{'neg': 0.714, 'neu': 0.286, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.3612</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.000</td>\n",
       "      <td>service suck</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>The shrimp tender and moist.</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>shrimp tender moist</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>There is not a deal good enough that would dra...</td>\n",
       "      <td>{'neg': 0.264, 'neu': 0.736, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.5116</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.000</td>\n",
       "      <td>deal good enough would drag establishment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Hard to judge whether these sides were good be...</td>\n",
       "      <td>{'neg': 0.297, 'neu': 0.621, 'pos': 0.082, 'co...</td>\n",
       "      <td>-0.6830</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.082</td>\n",
       "      <td>hard judge whether side good grossed melted st...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>On a positive note, our server was very attent...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.565, 'pos': 0.435, 'comp...</td>\n",
       "      <td>0.8271</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.435</td>\n",
       "      <td>positive note server attentive provided great ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Frozen pucks of disgust, with some of the wors...</td>\n",
       "      <td>{'neg': 0.421, 'neu': 0.579, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.8402</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.000</td>\n",
       "      <td>frozen puck disgust worst people behind register</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>The only thing I did like was the prime rib an...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.815, 'pos': 0.185, 'comp...</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.185</td>\n",
       "      <td>thing like prime rib dessert section</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>It's too bad the food is so damn generic.</td>\n",
       "      <td>{'neg': 0.5, 'neu': 0.5, 'pos': 0.0, 'compound...</td>\n",
       "      <td>-0.7900</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>bad food damn generic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>The burger is good beef, cooked just right.</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.707, 'pos': 0.293, 'comp...</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.293</td>\n",
       "      <td>burger good beef cooked right</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>If you want a sandwich just go to any Firehous...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.764, 'pos': 0.236, 'comp...</td>\n",
       "      <td>0.3544</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.236</td>\n",
       "      <td>want sandwich go firehouse</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>My side Greek salad with the Greek dressing wa...</td>\n",
       "      <td>{'neg': 0.159, 'neu': 0.841, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.5267</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.000</td>\n",
       "      <td>side greek salad greek dressing tasty pita hum...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>We ordered the duck rare and it was pink and t...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.872, 'pos': 0.128, 'comp...</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.128</td>\n",
       "      <td>ordered duck rare pink tender inside nice char...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Sentence  \\\n",
       "0                            Wow... Loved this place.   \n",
       "1                                  Crust is not good.   \n",
       "2           Not tasty and the texture was just nasty.   \n",
       "3   Stopped by during the late May bank holiday of...   \n",
       "4   The selection on the menu was great and so wer...   \n",
       "5      Now I am getting angry and I want my damn pho.   \n",
       "6               Honeslty it didn't taste THAT fresh.)   \n",
       "7   The potatoes were like rubber and you could te...   \n",
       "8                           The fries were great too.   \n",
       "9                                      A great touch.   \n",
       "10                           Service was very prompt.   \n",
       "11                                 Would not go back.   \n",
       "12  The cashier had no care what so ever on what I...   \n",
       "13  I tried the Cape Cod ravoli, chicken,with cran...   \n",
       "14  I was disgusted because I was pretty sure that...   \n",
       "15  I was shocked because no signs indicate cash o...   \n",
       "16                                Highly recommended.   \n",
       "17             Waitress was a little slow in service.   \n",
       "18  This place is not worth your time, let alone V...   \n",
       "19                               did not like at all.   \n",
       "20                                The Burrittos Blah!   \n",
       "21                                 The food, amazing.   \n",
       "22                              Service is also cute.   \n",
       "23  I could care less... The interior is just beau...   \n",
       "24                                 So they performed.   \n",
       "25  That's right....the red velvet cake.....ohhh t...   \n",
       "26  This hole in the wall has great Mexican street...   \n",
       "27  Took an hour to get our food only 4 tables in ...   \n",
       "28                  The worst was the salmon sashimi.   \n",
       "29  Also there are combos like a burger, fries, an...   \n",
       "30                      This was like the final blow!   \n",
       "31  I found this place by accident and I could not...   \n",
       "32  seems like a good quick place to grab a bite o...   \n",
       "33                  Overall, I like this place a lot.   \n",
       "34  The only redeeming quality of the restaurant w...   \n",
       "35                    Ample portions and good prices.   \n",
       "36  Poor service, the waiter made me feel like I w...   \n",
       "37              My first visit to Hiro was a delight!   \n",
       "38                                     Service sucks.   \n",
       "39                       The shrimp tender and moist.   \n",
       "40  There is not a deal good enough that would dra...   \n",
       "41  Hard to judge whether these sides were good be...   \n",
       "42  On a positive note, our server was very attent...   \n",
       "43  Frozen pucks of disgust, with some of the wors...   \n",
       "44  The only thing I did like was the prime rib an...   \n",
       "45          It's too bad the food is so damn generic.   \n",
       "46        The burger is good beef, cooked just right.   \n",
       "47  If you want a sandwich just go to any Firehous...   \n",
       "48  My side Greek salad with the Greek dressing wa...   \n",
       "49  We ordered the duck rare and it was pink and t...   \n",
       "\n",
       "                                               scores  compound_score  \\\n",
       "0   {'neg': 0.0, 'neu': 0.435, 'pos': 0.565, 'comp...          0.5994   \n",
       "1   {'neg': 0.445, 'neu': 0.555, 'pos': 0.0, 'comp...         -0.3412   \n",
       "2   {'neg': 0.34, 'neu': 0.66, 'pos': 0.0, 'compou...         -0.5574   \n",
       "3   {'neg': 0.093, 'neu': 0.585, 'pos': 0.322, 'co...          0.6908   \n",
       "4   {'neg': 0.0, 'neu': 0.728, 'pos': 0.272, 'comp...          0.6249   \n",
       "5   {'neg': 0.451, 'neu': 0.451, 'pos': 0.098, 'co...         -0.6908   \n",
       "6   {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...          0.0000   \n",
       "7   {'neg': 0.0, 'neu': 0.802, 'pos': 0.198, 'comp...          0.5719   \n",
       "8   {'neg': 0.0, 'neu': 0.494, 'pos': 0.506, 'comp...          0.6249   \n",
       "9   {'neg': 0.0, 'neu': 0.196, 'pos': 0.804, 'comp...          0.6249   \n",
       "10  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...          0.0000   \n",
       "11  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...          0.0000   \n",
       "12  {'neg': 0.094, 'neu': 0.769, 'pos': 0.137, 'co...          0.2500   \n",
       "13  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...          0.0000   \n",
       "14  {'neg': 0.214, 'neu': 0.44, 'pos': 0.346, 'com...          0.2732   \n",
       "15  {'neg': 0.429, 'neu': 0.571, 'pos': 0.0, 'comp...         -0.5423   \n",
       "16  {'neg': 0.0, 'neu': 0.323, 'pos': 0.677, 'comp...          0.2716   \n",
       "17  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...          0.0000   \n",
       "18  {'neg': 0.314, 'neu': 0.686, 'pos': 0.0, 'comp...         -0.3952   \n",
       "19  {'neg': 0.345, 'neu': 0.655, 'pos': 0.0, 'comp...         -0.2755   \n",
       "20  {'neg': 0.458, 'neu': 0.542, 'pos': 0.0, 'comp...         -0.1759   \n",
       "21  {'neg': 0.0, 'neu': 0.345, 'pos': 0.655, 'comp...          0.5859   \n",
       "22  {'neg': 0.0, 'neu': 0.5, 'pos': 0.5, 'compound...          0.4588   \n",
       "23  {'neg': 0.0, 'neu': 0.458, 'pos': 0.542, 'comp...          0.7964   \n",
       "24  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...          0.0000   \n",
       "25  {'neg': 0.0, 'neu': 0.706, 'pos': 0.294, 'comp...          0.5777   \n",
       "26  {'neg': 0.0, 'neu': 0.601, 'pos': 0.399, 'comp...          0.8074   \n",
       "27  {'neg': 0.0, 'neu': 0.796, 'pos': 0.204, 'comp...          0.5984   \n",
       "28  {'neg': 0.451, 'neu': 0.549, 'pos': 0.0, 'comp...         -0.6249   \n",
       "29  {'neg': 0.0, 'neu': 0.848, 'pos': 0.152, 'comp...          0.3612   \n",
       "30  {'neg': 0.0, 'neu': 0.642, 'pos': 0.358, 'comp...          0.4199   \n",
       "31  {'neg': 0.423, 'neu': 0.577, 'pos': 0.0, 'comp...         -0.7074   \n",
       "32  {'neg': 0.0, 'neu': 0.701, 'pos': 0.299, 'comp...          0.7391   \n",
       "33  {'neg': 0.0, 'neu': 0.615, 'pos': 0.385, 'comp...          0.3612   \n",
       "34  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...          0.0000   \n",
       "35  {'neg': 0.0, 'neu': 0.58, 'pos': 0.42, 'compou...          0.4404   \n",
       "36  {'neg': 0.283, 'neu': 0.609, 'pos': 0.109, 'co...         -0.6124   \n",
       "37  {'neg': 0.0, 'neu': 0.589, 'pos': 0.411, 'comp...          0.6360   \n",
       "38  {'neg': 0.714, 'neu': 0.286, 'pos': 0.0, 'comp...         -0.3612   \n",
       "39  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...          0.0000   \n",
       "40  {'neg': 0.264, 'neu': 0.736, 'pos': 0.0, 'comp...         -0.5116   \n",
       "41  {'neg': 0.297, 'neu': 0.621, 'pos': 0.082, 'co...         -0.6830   \n",
       "42  {'neg': 0.0, 'neu': 0.565, 'pos': 0.435, 'comp...          0.8271   \n",
       "43  {'neg': 0.421, 'neu': 0.579, 'pos': 0.0, 'comp...         -0.8402   \n",
       "44  {'neg': 0.0, 'neu': 0.815, 'pos': 0.185, 'comp...          0.3612   \n",
       "45  {'neg': 0.5, 'neu': 0.5, 'pos': 0.0, 'compound...         -0.7900   \n",
       "46  {'neg': 0.0, 'neu': 0.707, 'pos': 0.293, 'comp...          0.4404   \n",
       "47  {'neg': 0.0, 'neu': 0.764, 'pos': 0.236, 'comp...          0.3544   \n",
       "48  {'neg': 0.159, 'neu': 0.841, 'pos': 0.0, 'comp...         -0.5267   \n",
       "49  {'neg': 0.0, 'neu': 0.872, 'pos': 0.128, 'comp...          0.4215   \n",
       "\n",
       "    negative_score  positive_score  \\\n",
       "0            0.000           0.565   \n",
       "1            0.445           0.000   \n",
       "2            0.340           0.000   \n",
       "3            0.093           0.322   \n",
       "4            0.000           0.272   \n",
       "5            0.451           0.098   \n",
       "6            0.000           0.000   \n",
       "7            0.000           0.198   \n",
       "8            0.000           0.506   \n",
       "9            0.000           0.804   \n",
       "10           0.000           0.000   \n",
       "11           0.000           0.000   \n",
       "12           0.094           0.137   \n",
       "13           0.000           0.000   \n",
       "14           0.214           0.346   \n",
       "15           0.429           0.000   \n",
       "16           0.000           0.677   \n",
       "17           0.000           0.000   \n",
       "18           0.314           0.000   \n",
       "19           0.345           0.000   \n",
       "20           0.458           0.000   \n",
       "21           0.000           0.655   \n",
       "22           0.000           0.500   \n",
       "23           0.000           0.542   \n",
       "24           0.000           0.000   \n",
       "25           0.000           0.294   \n",
       "26           0.000           0.399   \n",
       "27           0.000           0.204   \n",
       "28           0.451           0.000   \n",
       "29           0.000           0.152   \n",
       "30           0.000           0.358   \n",
       "31           0.423           0.000   \n",
       "32           0.000           0.299   \n",
       "33           0.000           0.385   \n",
       "34           0.000           0.000   \n",
       "35           0.000           0.420   \n",
       "36           0.283           0.109   \n",
       "37           0.000           0.411   \n",
       "38           0.714           0.000   \n",
       "39           0.000           0.000   \n",
       "40           0.264           0.000   \n",
       "41           0.297           0.082   \n",
       "42           0.000           0.435   \n",
       "43           0.421           0.000   \n",
       "44           0.000           0.185   \n",
       "45           0.500           0.000   \n",
       "46           0.000           0.293   \n",
       "47           0.000           0.236   \n",
       "48           0.159           0.000   \n",
       "49           0.000           0.128   \n",
       "\n",
       "                                        processedText  topicmodel  \n",
       "0                                     wow loved place           2  \n",
       "1                                          crust good           2  \n",
       "2                                 tasty texture nasty           0  \n",
       "3   stopped late may bank holiday rick steve recom...           2  \n",
       "4                          selection menu great price           0  \n",
       "5                         getting angry want damn pho           0  \n",
       "6                          honeslty didnt taste fresh           1  \n",
       "7   potato like rubber could tell made ahead time ...           3  \n",
       "8                                           fry great           0  \n",
       "9                                         great touch           0  \n",
       "10                                     service prompt           0  \n",
       "11                                      would go back           0  \n",
       "12  cashier care ever say still ended wayyy overpr...           0  \n",
       "13    tried cape cod ravoli chickenwith cranberrymmmm           0  \n",
       "14                   disgusted pretty sure human hair           2  \n",
       "15                         shocked sign indicate cash           0  \n",
       "16                                 highly recommended           2  \n",
       "17                       waitress little slow service           0  \n",
       "18                    place worth time let alone vega           0  \n",
       "19                                               like           1  \n",
       "20                                     burrittos blah           3  \n",
       "21                                       food amazing           0  \n",
       "22                                  service also cute           1  \n",
       "23                   could care le interior beautiful           1  \n",
       "24                                          performed           3  \n",
       "25      thats rightthe red velvet cakeohhh stuff good           0  \n",
       "26  hole wall great mexican street taco friendly s...           0  \n",
       "27  took hour get food table restaurant food luke ...           0  \n",
       "28                               worst salmon sashimi           0  \n",
       "29        also combo like burger fry beer decent deal           1  \n",
       "30                                    like final blow           0  \n",
       "31                 found place accident could happier           3  \n",
       "32  seems like good quick place grab bite familiar...           0  \n",
       "33                             overall like place lot           1  \n",
       "34           redeeming quality restaurant inexpensive           3  \n",
       "35                           ample portion good price           0  \n",
       "36  poor service waiter made feel like stupid ever...           0  \n",
       "37                           first visit hiro delight           0  \n",
       "38                                       service suck           0  \n",
       "39                                shrimp tender moist           2  \n",
       "40          deal good enough would drag establishment           1  \n",
       "41  hard judge whether side good grossed melted st...           0  \n",
       "42  positive note server attentive provided great ...           0  \n",
       "43   frozen puck disgust worst people behind register           0  \n",
       "44               thing like prime rib dessert section           1  \n",
       "45                              bad food damn generic           0  \n",
       "46                      burger good beef cooked right           1  \n",
       "47                         want sandwich go firehouse           0  \n",
       "48  side greek salad greek dressing tasty pita hum...           0  \n",
       "49  ordered duck rare pink tender inside nice char...           3  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the Count Vectorized sparse matrix to an array\n",
    "X_array = count_vect.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check type of X_array\n",
    "type(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2396, 3628)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of X_array\n",
    "X_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3618</th>\n",
       "      <th>3619</th>\n",
       "      <th>3620</th>\n",
       "      <th>3621</th>\n",
       "      <th>3622</th>\n",
       "      <th>3623</th>\n",
       "      <th>3624</th>\n",
       "      <th>3625</th>\n",
       "      <th>3626</th>\n",
       "      <th>3627</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2391</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2392</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2393</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2394</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2395</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2396 rows × 3628 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3     4     5     6     7     8     9     ...  3618  \\\n",
       "0        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "1        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "2        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "3        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "4        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "2391     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "2392     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "2393     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "2394     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "2395     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "\n",
       "      3619  3620  3621  3622  3623  3624  3625  3626  3627  \n",
       "0        0     0     0     0     0     0     0     0     0  \n",
       "1        0     0     0     0     0     0     0     0     0  \n",
       "2        0     0     0     0     0     0     0     0     0  \n",
       "3        0     0     0     0     0     0     0     0     0  \n",
       "4        0     0     0     0     0     0     0     0     0  \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "2391     0     0     0     0     0     0     0     0     0  \n",
       "2392     0     0     0     0     0     0     0     0     0  \n",
       "2393     0     0     0     0     0     0     0     0     0  \n",
       "2394     0     0     0     0     0     0     0     0     0  \n",
       "2395     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[2396 rows x 3628 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert X_array into pandas dataframe\n",
    "df = pd.DataFrame(X_array)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2396, 3628)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign column names to df\n",
    "df.columns = cv.get_feature_names()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>scores</th>\n",
       "      <th>compound_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>processedText</th>\n",
       "      <th>topicmodel</th>\n",
       "      <th>abhor</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>...</th>\n",
       "      <th>young</th>\n",
       "      <th>youre</th>\n",
       "      <th>youthful</th>\n",
       "      <th>yucky</th>\n",
       "      <th>yukon</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "      <th>za</th>\n",
       "      <th>zero</th>\n",
       "      <th>zombiez</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.435, 'pos': 0.565, 'comp...</td>\n",
       "      <td>0.5994</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.565</td>\n",
       "      <td>wow loved place</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>{'neg': 0.445, 'neu': 0.555, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.3412</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.000</td>\n",
       "      <td>crust good</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>{'neg': 0.34, 'neu': 0.66, 'pos': 0.0, 'compou...</td>\n",
       "      <td>-0.5574</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.000</td>\n",
       "      <td>tasty texture nasty</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>{'neg': 0.093, 'neu': 0.585, 'pos': 0.322, 'co...</td>\n",
       "      <td>0.6908</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.322</td>\n",
       "      <td>stopped late may bank holiday rick steve recom...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.728, 'pos': 0.272, 'comp...</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.272</td>\n",
       "      <td>selection menu great price</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2391</th>\n",
       "      <td>Almost all of the songs in Cover Girl are old-...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>almost song cover girl oldfashioned tuneful</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2392</th>\n",
       "      <td>The most annoying thing about 'Cover Girl' is ...</td>\n",
       "      <td>{'neg': 0.143, 'neu': 0.857, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.4576</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.000</td>\n",
       "      <td>annoying thing cover girl way rita hayworth pu...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2393</th>\n",
       "      <td>Unfortunately, 'Cover Girl' is an example of h...</td>\n",
       "      <td>{'neg': 0.188, 'neu': 0.644, 'pos': 0.168, 'co...</td>\n",
       "      <td>0.1531</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.168</td>\n",
       "      <td>unfortunately cover girl example hollywood use...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2394</th>\n",
       "      <td>Non-linear narration thus many flashbacks and ...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.821, 'pos': 0.179, 'comp...</td>\n",
       "      <td>0.3384</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.179</td>\n",
       "      <td>nonlinear narration thus many flashback every ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2395</th>\n",
       "      <td>The good cinematography also makes her and Mon...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.585, 'pos': 0.415, 'comp...</td>\n",
       "      <td>0.7960</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.415</td>\n",
       "      <td>good cinematography also make monica bellucci ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2396 rows × 3635 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sentence  \\\n",
       "0                              Wow... Loved this place.   \n",
       "1                                    Crust is not good.   \n",
       "2             Not tasty and the texture was just nasty.   \n",
       "3     Stopped by during the late May bank holiday of...   \n",
       "4     The selection on the menu was great and so wer...   \n",
       "...                                                 ...   \n",
       "2391  Almost all of the songs in Cover Girl are old-...   \n",
       "2392  The most annoying thing about 'Cover Girl' is ...   \n",
       "2393  Unfortunately, 'Cover Girl' is an example of h...   \n",
       "2394  Non-linear narration thus many flashbacks and ...   \n",
       "2395  The good cinematography also makes her and Mon...   \n",
       "\n",
       "                                                 scores  compound_score  \\\n",
       "0     {'neg': 0.0, 'neu': 0.435, 'pos': 0.565, 'comp...          0.5994   \n",
       "1     {'neg': 0.445, 'neu': 0.555, 'pos': 0.0, 'comp...         -0.3412   \n",
       "2     {'neg': 0.34, 'neu': 0.66, 'pos': 0.0, 'compou...         -0.5574   \n",
       "3     {'neg': 0.093, 'neu': 0.585, 'pos': 0.322, 'co...          0.6908   \n",
       "4     {'neg': 0.0, 'neu': 0.728, 'pos': 0.272, 'comp...          0.6249   \n",
       "...                                                 ...             ...   \n",
       "2391  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...          0.0000   \n",
       "2392  {'neg': 0.143, 'neu': 0.857, 'pos': 0.0, 'comp...         -0.4576   \n",
       "2393  {'neg': 0.188, 'neu': 0.644, 'pos': 0.168, 'co...          0.1531   \n",
       "2394  {'neg': 0.0, 'neu': 0.821, 'pos': 0.179, 'comp...          0.3384   \n",
       "2395  {'neg': 0.0, 'neu': 0.585, 'pos': 0.415, 'comp...          0.7960   \n",
       "\n",
       "      negative_score  positive_score  \\\n",
       "0              0.000           0.565   \n",
       "1              0.445           0.000   \n",
       "2              0.340           0.000   \n",
       "3              0.093           0.322   \n",
       "4              0.000           0.272   \n",
       "...              ...             ...   \n",
       "2391           0.000           0.000   \n",
       "2392           0.143           0.000   \n",
       "2393           0.188           0.168   \n",
       "2394           0.000           0.179   \n",
       "2395           0.000           0.415   \n",
       "\n",
       "                                          processedText  topicmodel  abhor  \\\n",
       "0                                       wow loved place           2      0   \n",
       "1                                            crust good           2      0   \n",
       "2                                   tasty texture nasty           0      0   \n",
       "3     stopped late may bank holiday rick steve recom...           2      0   \n",
       "4                            selection menu great price           0      0   \n",
       "...                                                 ...         ...    ...   \n",
       "2391        almost song cover girl oldfashioned tuneful           1      0   \n",
       "2392  annoying thing cover girl way rita hayworth pu...           3      0   \n",
       "2393  unfortunately cover girl example hollywood use...           1      0   \n",
       "2394  nonlinear narration thus many flashback every ...           2      0   \n",
       "2395  good cinematography also make monica bellucci ...           1      0   \n",
       "\n",
       "      ability  able  ...  young  youre  youthful  yucky  yukon  yum  yummy  \\\n",
       "0           0     0  ...      0      0         0      0      0    0      0   \n",
       "1           0     0  ...      0      0         0      0      0    0      0   \n",
       "2           0     0  ...      0      0         0      0      0    0      0   \n",
       "3           0     0  ...      0      0         0      0      0    0      0   \n",
       "4           0     0  ...      0      0         0      0      0    0      0   \n",
       "...       ...   ...  ...    ...    ...       ...    ...    ...  ...    ...   \n",
       "2391        0     0  ...      0      0         0      0      0    0      0   \n",
       "2392        0     0  ...      0      0         0      0      0    0      0   \n",
       "2393        0     0  ...      0      0         0      0      0    0      0   \n",
       "2394        0     0  ...      0      0         0      0      0    0      0   \n",
       "2395        0     0  ...      0      0         0      0      0    0      0   \n",
       "\n",
       "      za  zero  zombiez  \n",
       "0      0     0        0  \n",
       "1      0     0        0  \n",
       "2      0     0        0  \n",
       "3      0     0        0  \n",
       "4      0     0        0  \n",
       "...   ..   ...      ...  \n",
       "2391   0     0        0  \n",
       "2392   0     0        0  \n",
       "2393   0     0        0  \n",
       "2394   0     0        0  \n",
       "2395   0     0        0  \n",
       "\n",
       "[2396 rows x 3635 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Concat these features in the original data\n",
    "new_data = pd.concat([X,df], axis=1)\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping all the text columns from the dataframe\n",
    "new_data = new_data.drop([\"Sentence\",\"scores\",\"processedText\"],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compound_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>topicmodel</th>\n",
       "      <th>abhor</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abound</th>\n",
       "      <th>abovepretty</th>\n",
       "      <th>absolute</th>\n",
       "      <th>...</th>\n",
       "      <th>young</th>\n",
       "      <th>youre</th>\n",
       "      <th>youthful</th>\n",
       "      <th>yucky</th>\n",
       "      <th>yukon</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "      <th>za</th>\n",
       "      <th>zero</th>\n",
       "      <th>zombiez</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5994</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.565</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.3412</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.5574</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.6908</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.322</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6249</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3632 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   compound_score  negative_score  positive_score  topicmodel  abhor  ability  \\\n",
       "0          0.5994           0.000           0.565           2      0        0   \n",
       "1         -0.3412           0.445           0.000           2      0        0   \n",
       "2         -0.5574           0.340           0.000           0      0        0   \n",
       "3          0.6908           0.093           0.322           2      0        0   \n",
       "4          0.6249           0.000           0.272           0      0        0   \n",
       "\n",
       "   able  abound  abovepretty  absolute  ...  young  youre  youthful  yucky  \\\n",
       "0     0       0            0         0  ...      0      0         0      0   \n",
       "1     0       0            0         0  ...      0      0         0      0   \n",
       "2     0       0            0         0  ...      0      0         0      0   \n",
       "3     0       0            0         0  ...      0      0         0      0   \n",
       "4     0       0            0         0  ...      0      0         0      0   \n",
       "\n",
       "   yukon  yum  yummy  za  zero  zombiez  \n",
       "0      0    0      0   0     0        0  \n",
       "1      0    0      0   0     0        0  \n",
       "2      0    0      0   0     0        0  \n",
       "3      0    0      0   0     0        0  \n",
       "4      0    0      0   0     0        0  \n",
       "\n",
       "[5 rows x 3632 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.b. Modeling\n",
    "\n",
    "Use your favorite ML algorithm to train a classification model.  Don’t forget everything that we’ve learned in our ML course: hyperparameter tuning, cross validation, handling imbalanced data, etc. Make reasonable decisions and try to create the best-performing classifier that you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Logistic Regression model for prediction\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperTuning Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hypertuning the regularization  strength 'C' and the \"Solver' parameters to create the best performing LR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'C':[0,0.01,0.1,1,10,100],\n",
    "         'solver': [\"lbfgs\",\"liblinear\",\"newton-cg\",\"sag\",\"saga\"]\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Cross Validation instance using accuracy as the metrics\n",
    "grid = GridSearchCV(lr,param,cv = 5,scoring = \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1407, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 759, in _logistic_regression_path\n",
      "    args=(X, target, 1. / C, sample_weight),\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1407, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 759, in _logistic_regression_path\n",
      "    args=(X, target, 1. / C, sample_weight),\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1407, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 759, in _logistic_regression_path\n",
      "    args=(X, target, 1. / C, sample_weight),\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1407, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 759, in _logistic_regression_path\n",
      "    args=(X, target, 1. / C, sample_weight),\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1407, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 759, in _logistic_regression_path\n",
      "    args=(X, target, 1. / C, sample_weight),\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1356, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 966, in _fit_liblinear\n",
      "    raw_coef_, n_iter_ = liblinear.train_wrap(\n",
      "  File \"sklearn\\svm\\_liblinear.pyx\", line 52, in sklearn.svm._liblinear.train_wrap\n",
      "ValueError: b'C <= 0'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1356, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 966, in _fit_liblinear\n",
      "    raw_coef_, n_iter_ = liblinear.train_wrap(\n",
      "  File \"sklearn\\svm\\_liblinear.pyx\", line 52, in sklearn.svm._liblinear.train_wrap\n",
      "ValueError: b'C <= 0'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1356, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 966, in _fit_liblinear\n",
      "    raw_coef_, n_iter_ = liblinear.train_wrap(\n",
      "  File \"sklearn\\svm\\_liblinear.pyx\", line 52, in sklearn.svm._liblinear.train_wrap\n",
      "ValueError: b'C <= 0'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1356, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 966, in _fit_liblinear\n",
      "    raw_coef_, n_iter_ = liblinear.train_wrap(\n",
      "  File \"sklearn\\svm\\_liblinear.pyx\", line 52, in sklearn.svm._liblinear.train_wrap\n",
      "ValueError: b'C <= 0'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1356, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 966, in _fit_liblinear\n",
      "    raw_coef_, n_iter_ = liblinear.train_wrap(\n",
      "  File \"sklearn\\svm\\_liblinear.pyx\", line 52, in sklearn.svm._liblinear.train_wrap\n",
      "ValueError: b'C <= 0'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1407, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 767, in _logistic_regression_path\n",
      "    args = (X, target, 1. / C, sample_weight)\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1407, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 767, in _logistic_regression_path\n",
      "    args = (X, target, 1. / C, sample_weight)\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1407, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 767, in _logistic_regression_path\n",
      "    args = (X, target, 1. / C, sample_weight)\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1407, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 767, in _logistic_regression_path\n",
      "    args = (X, target, 1. / C, sample_weight)\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1407, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 767, in _logistic_regression_path\n",
      "    args = (X, target, 1. / C, sample_weight)\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1407, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 791, in _logistic_regression_path\n",
      "    alpha = 1. / C\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1407, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 791, in _logistic_regression_path\n",
      "    alpha = 1. / C\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1407, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 791, in _logistic_regression_path\n",
      "    alpha = 1. / C\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1407, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 791, in _logistic_regression_path\n",
      "    alpha = 1. / C\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1407, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 791, in _logistic_regression_path\n",
      "    alpha = 1. / C\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1407, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 791, in _logistic_regression_path\n",
      "    alpha = 1. / C\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1407, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 791, in _logistic_regression_path\n",
      "    alpha = 1. / C\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1407, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 791, in _logistic_regression_path\n",
      "    alpha = 1. / C\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1407, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 791, in _logistic_regression_path\n",
      "    alpha = 1. / C\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1407, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 791, in _logistic_regression_path\n",
      "    alpha = 1. / C\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\ishik\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(random_state=16),\n",
       "             param_grid={'C': [0, 0.01, 0.1, 1, 10, 100],\n",
       "                         'solver': ['lbfgs', 'liblinear', 'newton-cg', 'sag',\n",
       "                                    'saga']},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(new_data,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8459959986082115"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Deriving the accuracy score of the best model\n",
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 100, 'solver': 'saga'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Parameters of the best model\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.c. Assessing\n",
    "\n",
    "Use the testing data to measure the accuracy and F1-score of your model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Sentence, Polarity]\n",
       "Index: []"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Making changes to the testing data\n",
    "#Check for missing data\n",
    "df_test[df_test[\"Sentence\"] ==\"#NAME?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence    0\n",
       "Polarity    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating scores column by applying sentiment analysis\n",
    "df_test[\"scores\"] = df_test[\"Sentence\"].apply(lambda pappu:sid.polarity_scores(pappu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a compound score feature on the test dataset\n",
    "df_test[\"compound_score\"] = df_test[\"scores\"].apply(lambda di:di[\"compound\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating negative score column\n",
    "df_test[\"negative_score\"] = df_test[\"scores\"].apply(lambda di:di[\"neg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating positive score column\n",
    "df_test[\"positive_score\"] = df_test[\"scores\"].apply(lambda di:di[\"pos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating neutral score column\n",
    "#df_test[\"neutral_score\"] = df_test[\"scores\"].apply(lambda di:di[\"neu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>scores</th>\n",
       "      <th>compound_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>positive_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A good commentary of today's love and undoubte...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.438, 'pos': 0.562, 'comp...</td>\n",
       "      <td>0.8402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For people who are first timers in film making...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'comp...</td>\n",
       "      <td>0.6467</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It was very popular when I was in the cinema, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.554, 'pos': 0.446, 'comp...</td>\n",
       "      <td>0.9020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's a feel-good film and that's how I felt wh...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It has northern humour and positive about the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.573, 'pos': 0.427, 'comp...</td>\n",
       "      <td>0.7717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Polarity  \\\n",
       "0  A good commentary of today's love and undoubte...         1   \n",
       "1  For people who are first timers in film making...         1   \n",
       "2  It was very popular when I was in the cinema, ...         1   \n",
       "3  It's a feel-good film and that's how I felt wh...         1   \n",
       "4  It has northern humour and positive about the ...         1   \n",
       "\n",
       "                                              scores  compound_score  \\\n",
       "0  {'neg': 0.0, 'neu': 0.438, 'pos': 0.562, 'comp...          0.8402   \n",
       "1  {'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'comp...          0.6467   \n",
       "2  {'neg': 0.0, 'neu': 0.554, 'pos': 0.446, 'comp...          0.9020   \n",
       "3  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...          0.0000   \n",
       "4  {'neg': 0.0, 'neu': 0.573, 'pos': 0.427, 'comp...          0.7717   \n",
       "\n",
       "   negative_score  positive_score  \n",
       "0             0.0           0.562  \n",
       "1             0.0           0.234  \n",
       "2             0.0           0.446  \n",
       "3             0.0           0.000  \n",
       "4             0.0           0.427  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating X_test dataset\n",
    "X_test = df_test.drop('Polarity',axis = 1,inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating y_test dataset\n",
    "y_test = df_test['Polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the text cleaning function on the test dataset\n",
    "X_test['processedText'] = X_test['Sentence'].apply(lambda x: text_clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating text column with the processed data for creating count vectorizer\n",
    "test_text = X_test[\"processedText\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting Count Vectorizer on the test dataset\n",
    "test_vect = cv.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting LDA for topic modelling\n",
    "topic_results_test = LDA.transform(test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the topicmodel feature with the topic number of the highest weighted topic\n",
    "X_test[\"topicmodel\"] = topic_results_test.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>scores</th>\n",
       "      <th>compound_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>processedText</th>\n",
       "      <th>topicmodel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A good commentary of today's love and undoubte...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.438, 'pos': 0.562, 'comp...</td>\n",
       "      <td>0.8402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.562</td>\n",
       "      <td>good commentary today love undoubtedly film wo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For people who are first timers in film making...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'comp...</td>\n",
       "      <td>0.6467</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.234</td>\n",
       "      <td>people first timer film making think excellent...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It was very popular when I was in the cinema, ...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.554, 'pos': 0.446, 'comp...</td>\n",
       "      <td>0.9020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.446</td>\n",
       "      <td>popular cinema good house good reaction plenty...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's a feel-good film and that's how I felt wh...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>feelgood film thats felt came cinema</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It has northern humour and positive about the ...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.573, 'pos': 0.427, 'comp...</td>\n",
       "      <td>0.7717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.427</td>\n",
       "      <td>northern humour positive community represents</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I rather enjoyed it.</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.377, 'pos': 0.623, 'comp...</td>\n",
       "      <td>0.5106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.623</td>\n",
       "      <td>rather enjoyed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I liked it.</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.263, 'pos': 0.737, 'comp...</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.737</td>\n",
       "      <td>liked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I couldn't take them seriously.</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.664, 'pos': 0.336, 'comp...</td>\n",
       "      <td>0.1326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.336</td>\n",
       "      <td>couldnt take seriously</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>It really created a unique feeling though.</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.496, 'pos': 0.504, 'comp...</td>\n",
       "      <td>0.4690</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.504</td>\n",
       "      <td>really created unique feeling though</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vivian Schilling did an excellent job with the...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.684, 'pos': 0.316, 'comp...</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.316</td>\n",
       "      <td>vivian schilling excellent job script</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0  A good commentary of today's love and undoubte...   \n",
       "1  For people who are first timers in film making...   \n",
       "2  It was very popular when I was in the cinema, ...   \n",
       "3  It's a feel-good film and that's how I felt wh...   \n",
       "4  It has northern humour and positive about the ...   \n",
       "5                             I rather enjoyed it.     \n",
       "6                                      I liked it.     \n",
       "7                  I couldn't take them seriously.     \n",
       "8       It really created a unique feeling though.     \n",
       "9  Vivian Schilling did an excellent job with the...   \n",
       "\n",
       "                                              scores  compound_score  \\\n",
       "0  {'neg': 0.0, 'neu': 0.438, 'pos': 0.562, 'comp...          0.8402   \n",
       "1  {'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'comp...          0.6467   \n",
       "2  {'neg': 0.0, 'neu': 0.554, 'pos': 0.446, 'comp...          0.9020   \n",
       "3  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...          0.0000   \n",
       "4  {'neg': 0.0, 'neu': 0.573, 'pos': 0.427, 'comp...          0.7717   \n",
       "5  {'neg': 0.0, 'neu': 0.377, 'pos': 0.623, 'comp...          0.5106   \n",
       "6  {'neg': 0.0, 'neu': 0.263, 'pos': 0.737, 'comp...          0.4215   \n",
       "7  {'neg': 0.0, 'neu': 0.664, 'pos': 0.336, 'comp...          0.1326   \n",
       "8  {'neg': 0.0, 'neu': 0.496, 'pos': 0.504, 'comp...          0.4690   \n",
       "9  {'neg': 0.0, 'neu': 0.684, 'pos': 0.316, 'comp...          0.5719   \n",
       "\n",
       "   negative_score  positive_score  \\\n",
       "0             0.0           0.562   \n",
       "1             0.0           0.234   \n",
       "2             0.0           0.446   \n",
       "3             0.0           0.000   \n",
       "4             0.0           0.427   \n",
       "5             0.0           0.623   \n",
       "6             0.0           0.737   \n",
       "7             0.0           0.336   \n",
       "8             0.0           0.504   \n",
       "9             0.0           0.316   \n",
       "\n",
       "                                       processedText  topicmodel  \n",
       "0  good commentary today love undoubtedly film wo...           2  \n",
       "1  people first timer film making think excellent...           1  \n",
       "2  popular cinema good house good reaction plenty...           0  \n",
       "3               feelgood film thats felt came cinema           0  \n",
       "4      northern humour positive community represents           3  \n",
       "5                                     rather enjoyed           1  \n",
       "6                                              liked           1  \n",
       "7                             couldnt take seriously           0  \n",
       "8               really created unique feeling though           0  \n",
       "9              vivian schilling excellent job script           2  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting test_vect to array\n",
    "test_array = test_vect.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3618</th>\n",
       "      <th>3619</th>\n",
       "      <th>3620</th>\n",
       "      <th>3621</th>\n",
       "      <th>3622</th>\n",
       "      <th>3623</th>\n",
       "      <th>3624</th>\n",
       "      <th>3625</th>\n",
       "      <th>3626</th>\n",
       "      <th>3627</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 3628 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2     3     4     5     6     7     8     9     ...  3618  \\\n",
       "0       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "1       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "2       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "3       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "4       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "595     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "596     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "597     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "598     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "599     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "\n",
       "     3619  3620  3621  3622  3623  3624  3625  3626  3627  \n",
       "0       0     0     0     0     0     0     0     0     0  \n",
       "1       0     0     0     0     0     0     0     0     0  \n",
       "2       0     0     0     0     0     0     0     0     0  \n",
       "3       0     0     0     0     0     0     0     0     0  \n",
       "4       0     0     0     0     0     0     0     0     0  \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "595     0     0     0     0     0     0     0     0     0  \n",
       "596     0     0     0     0     0     0     0     0     0  \n",
       "597     0     0     0     0     0     0     0     0     0  \n",
       "598     0     0     0     0     0     0     0     0     0  \n",
       "599     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[600 rows x 3628 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert test_array into pandas dataframe\n",
    "testdf = pd.DataFrame(test_array)\n",
    "testdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 3628)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign column names to testdf\n",
    "testdf.columns = cv.get_feature_names()\n",
    "testdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>scores</th>\n",
       "      <th>compound_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>processedText</th>\n",
       "      <th>topicmodel</th>\n",
       "      <th>abhor</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>...</th>\n",
       "      <th>young</th>\n",
       "      <th>youre</th>\n",
       "      <th>youthful</th>\n",
       "      <th>yucky</th>\n",
       "      <th>yukon</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "      <th>za</th>\n",
       "      <th>zero</th>\n",
       "      <th>zombiez</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A good commentary of today's love and undoubte...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.438, 'pos': 0.562, 'comp...</td>\n",
       "      <td>0.8402</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.562</td>\n",
       "      <td>good commentary today love undoubtedly film wo...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For people who are first timers in film making...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'comp...</td>\n",
       "      <td>0.6467</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.234</td>\n",
       "      <td>people first timer film making think excellent...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It was very popular when I was in the cinema, ...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.554, 'pos': 0.446, 'comp...</td>\n",
       "      <td>0.9020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.446</td>\n",
       "      <td>popular cinema good house good reaction plenty...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's a feel-good film and that's how I felt wh...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>feelgood film thats felt came cinema</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It has northern humour and positive about the ...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.573, 'pos': 0.427, 'comp...</td>\n",
       "      <td>0.7717</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.427</td>\n",
       "      <td>northern humour positive community represents</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>I just got bored watching Jessice Lange take h...</td>\n",
       "      <td>{'neg': 0.21, 'neu': 0.79, 'pos': 0.0, 'compou...</td>\n",
       "      <td>-0.3382</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.000</td>\n",
       "      <td>got bored watching jessice lange take clothes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>Unfortunately, any virtue in this film's produ...</td>\n",
       "      <td>{'neg': 0.404, 'neu': 0.455, 'pos': 0.141, 'co...</td>\n",
       "      <td>-0.6369</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.141</td>\n",
       "      <td>unfortunately virtue film production work lost...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>In a word, it is embarrassing.</td>\n",
       "      <td>{'neg': 0.394, 'neu': 0.606, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.3818</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.000</td>\n",
       "      <td>word embarrassing</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>Exceptionally bad!</td>\n",
       "      <td>{'neg': 0.803, 'neu': 0.197, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.6230</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.000</td>\n",
       "      <td>exceptionally bad</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>All in all its an insult to one's intelligence...</td>\n",
       "      <td>{'neg': 0.284, 'neu': 0.465, 'pos': 0.251, 'co...</td>\n",
       "      <td>-0.1779</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.251</td>\n",
       "      <td>insult one intelligence huge waste money</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 3635 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sentence  \\\n",
       "0    A good commentary of today's love and undoubte...   \n",
       "1    For people who are first timers in film making...   \n",
       "2    It was very popular when I was in the cinema, ...   \n",
       "3    It's a feel-good film and that's how I felt wh...   \n",
       "4    It has northern humour and positive about the ...   \n",
       "..                                                 ...   \n",
       "595  I just got bored watching Jessice Lange take h...   \n",
       "596  Unfortunately, any virtue in this film's produ...   \n",
       "597                   In a word, it is embarrassing.     \n",
       "598                               Exceptionally bad!     \n",
       "599  All in all its an insult to one's intelligence...   \n",
       "\n",
       "                                                scores  compound_score  \\\n",
       "0    {'neg': 0.0, 'neu': 0.438, 'pos': 0.562, 'comp...          0.8402   \n",
       "1    {'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'comp...          0.6467   \n",
       "2    {'neg': 0.0, 'neu': 0.554, 'pos': 0.446, 'comp...          0.9020   \n",
       "3    {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...          0.0000   \n",
       "4    {'neg': 0.0, 'neu': 0.573, 'pos': 0.427, 'comp...          0.7717   \n",
       "..                                                 ...             ...   \n",
       "595  {'neg': 0.21, 'neu': 0.79, 'pos': 0.0, 'compou...         -0.3382   \n",
       "596  {'neg': 0.404, 'neu': 0.455, 'pos': 0.141, 'co...         -0.6369   \n",
       "597  {'neg': 0.394, 'neu': 0.606, 'pos': 0.0, 'comp...         -0.3818   \n",
       "598  {'neg': 0.803, 'neu': 0.197, 'pos': 0.0, 'comp...         -0.6230   \n",
       "599  {'neg': 0.284, 'neu': 0.465, 'pos': 0.251, 'co...         -0.1779   \n",
       "\n",
       "     negative_score  positive_score  \\\n",
       "0             0.000           0.562   \n",
       "1             0.000           0.234   \n",
       "2             0.000           0.446   \n",
       "3             0.000           0.000   \n",
       "4             0.000           0.427   \n",
       "..              ...             ...   \n",
       "595           0.210           0.000   \n",
       "596           0.404           0.141   \n",
       "597           0.394           0.000   \n",
       "598           0.803           0.000   \n",
       "599           0.284           0.251   \n",
       "\n",
       "                                         processedText  topicmodel  abhor  \\\n",
       "0    good commentary today love undoubtedly film wo...           2      0   \n",
       "1    people first timer film making think excellent...           1      0   \n",
       "2    popular cinema good house good reaction plenty...           0      0   \n",
       "3                 feelgood film thats felt came cinema           0      0   \n",
       "4        northern humour positive community represents           3      0   \n",
       "..                                                 ...         ...    ...   \n",
       "595      got bored watching jessice lange take clothes           0      0   \n",
       "596  unfortunately virtue film production work lost...           1      0   \n",
       "597                                  word embarrassing           2      0   \n",
       "598                                  exceptionally bad           3      0   \n",
       "599           insult one intelligence huge waste money           2      0   \n",
       "\n",
       "     ability  able  ...  young  youre  youthful  yucky  yukon  yum  yummy  za  \\\n",
       "0          0     0  ...      0      0         0      0      0    0      0   0   \n",
       "1          0     0  ...      0      0         0      0      0    0      0   0   \n",
       "2          0     0  ...      0      0         0      0      0    0      0   0   \n",
       "3          0     0  ...      0      0         0      0      0    0      0   0   \n",
       "4          0     0  ...      0      0         0      0      0    0      0   0   \n",
       "..       ...   ...  ...    ...    ...       ...    ...    ...  ...    ...  ..   \n",
       "595        0     0  ...      0      0         0      0      0    0      0   0   \n",
       "596        0     0  ...      0      0         0      0      0    0      0   0   \n",
       "597        0     0  ...      0      0         0      0      0    0      0   0   \n",
       "598        0     0  ...      0      0         0      0      0    0      0   0   \n",
       "599        0     0  ...      0      0         0      0      0    0      0   0   \n",
       "\n",
       "     zero  zombiez  \n",
       "0       0        0  \n",
       "1       0        0  \n",
       "2       0        0  \n",
       "3       0        0  \n",
       "4       0        0  \n",
       "..    ...      ...  \n",
       "595     0        0  \n",
       "596     0        0  \n",
       "597     0        0  \n",
       "598     0        0  \n",
       "599     0        0  \n",
       "\n",
       "[600 rows x 3635 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## You can now concat these features in the original data\n",
    "test_new_data = pd.concat([X_test,testdf], axis=1)\n",
    "test_new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing text columns from the dataset before applying the logistic regression model\n",
    "test_new_data = test_new_data.drop([\"Sentence\",\"scores\",\"processedText\"],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Model on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making prediction on the test dataset using the best model\n",
    "predictions = grid.predict(test_new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics: Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.89      0.82       287\n",
      "           1       0.88      0.74      0.80       313\n",
      "\n",
      "    accuracy                           0.81       600\n",
      "   macro avg       0.82      0.81      0.81       600\n",
      "weighted avg       0.82      0.81      0.81       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics: Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[254  33]\n",
      " [ 82 231]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a conda environment\n",
    "#from pycaret.classification import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_pyc = new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_pyc[\"Polarity\"] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_pyc.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s = setup(x_pyc, target =\"Polarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cl = compare_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Given the accuracy and F1-score of your model, are you satisfied with the results, from a business point of view? Explain."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polarity in the dataset states whether a review is negative or positive and for any business, it is more important to focus on the negative reviews with the polarity 0 rather than the reviews with polarity 1. As these negative reviews will enable them to improve their process or understand the overall customer sentiment.\n",
    "The accuracy of the model is 81% and F1-score of  ‘0’ polarity is 82%. Given the fact that the dataset is balanced, accuracy of 81% makes for a good confidence in the model. It is to say that model will predict the polarity of the reviews with 81% confidence. Therefore, we are capturing most of the correct results in our reviews.\n",
    "Another aspect that makes it a good model is F1-score which is 82%. It is a mean (Harmonic Mean to be specific) of two other measures Precision and Recall. And interestingly, the Recall of our model is 89%. This is to say, our model will tag all the negative polarity reviews with 89% confidence, leaving only 11% chance of Type 2 error, so when management is looking at the 0 predicted reviews, they can be utmost confident of the result.\n",
    "It is important for the recall to be high because it will ensure that management is not wasting their time on incorrect reviews.\n",
    "\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Show five example instances in which your model’s predictions were incorrect. Describe why you think the model was wrong. Don’t just guess: dig deep to figure out the root cause."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Predicted Value with the Test DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predictions is an array type\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe of \"predictions\" so that it can be joined with the test dataframe\n",
    "preddf = pd.DataFrame(data = predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  1\n",
       "1  1\n",
       "2  1"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preddf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dffinal = X_test.join(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>scores</th>\n",
       "      <th>compound_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>processedText</th>\n",
       "      <th>topicmodel</th>\n",
       "      <th>Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A good commentary of today's love and undoubte...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.438, 'pos': 0.562, 'comp...</td>\n",
       "      <td>0.8402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.562</td>\n",
       "      <td>good commentary today love undoubtedly film wo...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For people who are first timers in film making...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'comp...</td>\n",
       "      <td>0.6467</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.234</td>\n",
       "      <td>people first timer film making think excellent...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It was very popular when I was in the cinema, ...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.554, 'pos': 0.446, 'comp...</td>\n",
       "      <td>0.9020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.446</td>\n",
       "      <td>popular cinema good house good reaction plenty...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's a feel-good film and that's how I felt wh...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>feelgood film thats felt came cinema</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It has northern humour and positive about the ...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.573, 'pos': 0.427, 'comp...</td>\n",
       "      <td>0.7717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.427</td>\n",
       "      <td>northern humour positive community represents</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0  A good commentary of today's love and undoubte...   \n",
       "1  For people who are first timers in film making...   \n",
       "2  It was very popular when I was in the cinema, ...   \n",
       "3  It's a feel-good film and that's how I felt wh...   \n",
       "4  It has northern humour and positive about the ...   \n",
       "\n",
       "                                              scores  compound_score  \\\n",
       "0  {'neg': 0.0, 'neu': 0.438, 'pos': 0.562, 'comp...          0.8402   \n",
       "1  {'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'comp...          0.6467   \n",
       "2  {'neg': 0.0, 'neu': 0.554, 'pos': 0.446, 'comp...          0.9020   \n",
       "3  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...          0.0000   \n",
       "4  {'neg': 0.0, 'neu': 0.573, 'pos': 0.427, 'comp...          0.7717   \n",
       "\n",
       "   negative_score  positive_score  \\\n",
       "0             0.0           0.562   \n",
       "1             0.0           0.234   \n",
       "2             0.0           0.446   \n",
       "3             0.0           0.000   \n",
       "4             0.0           0.427   \n",
       "\n",
       "                                       processedText  topicmodel  Polarity  \n",
       "0  good commentary today love undoubtedly film wo...           2         1  \n",
       "1  people first timer film making think excellent...           1         1  \n",
       "2  popular cinema good house good reaction plenty...           0         1  \n",
       "3               feelgood film thats felt came cinema           0         1  \n",
       "4      northern humour positive community represents           3         1  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dffinal.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining the \"Predicted Value\" with the test dataframe\n",
    "#The test data frame to which the feature is joined is already preprocessed\n",
    "dffinal = dffinal.join(preddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>scores</th>\n",
       "      <th>compound_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>processedText</th>\n",
       "      <th>topicmodel</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A good commentary of today's love and undoubte...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.438, 'pos': 0.562, 'comp...</td>\n",
       "      <td>0.8402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.562</td>\n",
       "      <td>good commentary today love undoubtedly film wo...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For people who are first timers in film making...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'comp...</td>\n",
       "      <td>0.6467</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.234</td>\n",
       "      <td>people first timer film making think excellent...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It was very popular when I was in the cinema, ...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.554, 'pos': 0.446, 'comp...</td>\n",
       "      <td>0.9020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.446</td>\n",
       "      <td>popular cinema good house good reaction plenty...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's a feel-good film and that's how I felt wh...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>feelgood film thats felt came cinema</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It has northern humour and positive about the ...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.573, 'pos': 0.427, 'comp...</td>\n",
       "      <td>0.7717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.427</td>\n",
       "      <td>northern humour positive community represents</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0  A good commentary of today's love and undoubte...   \n",
       "1  For people who are first timers in film making...   \n",
       "2  It was very popular when I was in the cinema, ...   \n",
       "3  It's a feel-good film and that's how I felt wh...   \n",
       "4  It has northern humour and positive about the ...   \n",
       "\n",
       "                                              scores  compound_score  \\\n",
       "0  {'neg': 0.0, 'neu': 0.438, 'pos': 0.562, 'comp...          0.8402   \n",
       "1  {'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'comp...          0.6467   \n",
       "2  {'neg': 0.0, 'neu': 0.554, 'pos': 0.446, 'comp...          0.9020   \n",
       "3  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...          0.0000   \n",
       "4  {'neg': 0.0, 'neu': 0.573, 'pos': 0.427, 'comp...          0.7717   \n",
       "\n",
       "   negative_score  positive_score  \\\n",
       "0             0.0           0.562   \n",
       "1             0.0           0.234   \n",
       "2             0.0           0.446   \n",
       "3             0.0           0.000   \n",
       "4             0.0           0.427   \n",
       "\n",
       "                                       processedText  topicmodel  Polarity  0  \n",
       "0  good commentary today love undoubtedly film wo...           2         1  1  \n",
       "1  people first timer film making think excellent...           1         1  1  \n",
       "2  popular cinema good house good reaction plenty...           0         1  1  \n",
       "3               feelgood film thats felt came cinema           0         1  0  \n",
       "4      northern humour positive community represents           3         1  1  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Final Dataframe\n",
    "dffinal.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming the column of the 'Predicted Value'\n",
    "dffinal = dffinal.rename(columns={0:\"Predicted\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>scores</th>\n",
       "      <th>compound_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>processedText</th>\n",
       "      <th>topicmodel</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A good commentary of today's love and undoubte...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.438, 'pos': 0.562, 'comp...</td>\n",
       "      <td>0.8402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.562</td>\n",
       "      <td>good commentary today love undoubtedly film wo...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For people who are first timers in film making...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'comp...</td>\n",
       "      <td>0.6467</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.234</td>\n",
       "      <td>people first timer film making think excellent...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It was very popular when I was in the cinema, ...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.554, 'pos': 0.446, 'comp...</td>\n",
       "      <td>0.9020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.446</td>\n",
       "      <td>popular cinema good house good reaction plenty...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's a feel-good film and that's how I felt wh...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>feelgood film thats felt came cinema</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It has northern humour and positive about the ...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.573, 'pos': 0.427, 'comp...</td>\n",
       "      <td>0.7717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.427</td>\n",
       "      <td>northern humour positive community represents</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0  A good commentary of today's love and undoubte...   \n",
       "1  For people who are first timers in film making...   \n",
       "2  It was very popular when I was in the cinema, ...   \n",
       "3  It's a feel-good film and that's how I felt wh...   \n",
       "4  It has northern humour and positive about the ...   \n",
       "\n",
       "                                              scores  compound_score  \\\n",
       "0  {'neg': 0.0, 'neu': 0.438, 'pos': 0.562, 'comp...          0.8402   \n",
       "1  {'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'comp...          0.6467   \n",
       "2  {'neg': 0.0, 'neu': 0.554, 'pos': 0.446, 'comp...          0.9020   \n",
       "3  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...          0.0000   \n",
       "4  {'neg': 0.0, 'neu': 0.573, 'pos': 0.427, 'comp...          0.7717   \n",
       "\n",
       "   negative_score  positive_score  \\\n",
       "0             0.0           0.562   \n",
       "1             0.0           0.234   \n",
       "2             0.0           0.446   \n",
       "3             0.0           0.000   \n",
       "4             0.0           0.427   \n",
       "\n",
       "                                       processedText  topicmodel  Polarity  \\\n",
       "0  good commentary today love undoubtedly film wo...           2         1   \n",
       "1  people first timer film making think excellent...           1         1   \n",
       "2  popular cinema good house good reaction plenty...           0         1   \n",
       "3               feelgood film thats felt came cinema           0         1   \n",
       "4      northern humour positive community represents           3         1   \n",
       "\n",
       "   Predicted  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          0  \n",
       "4          1  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dffinal.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new feature having the neutral sentiment value.\n",
    "#This feature was not included while training the model\n",
    "#To determine the reason behind the wrong prediction, this feature can provide valuable insights\n",
    "dffinal[\"neutral_score\"] = dffinal[\"scores\"].apply(lambda di:di[\"neu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe where the Predicted values did not match the actual values\n",
    "df_mismatch = dffinal[dffinal[\"Polarity\"] != dffinal[\"Predicted\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Five Example instances with incorrect model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>scores</th>\n",
       "      <th>compound_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>processedText</th>\n",
       "      <th>topicmodel</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>neutral_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's a feel-good film and that's how I felt wh...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>feelgood film thats felt came cinema</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I couldn't take them seriously.</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.664, 'pos': 0.336, 'comp...</td>\n",
       "      <td>0.1326</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.336</td>\n",
       "      <td>couldnt take seriously</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Not too screamy not to masculine but just righ...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>screamy masculine right</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>This movie is excellent!Angel is beautiful and...</td>\n",
       "      <td>{'neg': 0.019, 'neu': 0.789, 'pos': 0.191, 'co...</td>\n",
       "      <td>0.9218</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.191</td>\n",
       "      <td>movie excellentangel beautiful scamp adorableh...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>The soundtrack wasn't terrible, either.</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.61, 'pos': 0.39, 'compou...</td>\n",
       "      <td>0.3724</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.390</td>\n",
       "      <td>soundtrack wasnt terrible either</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Sentence  \\\n",
       "3   It's a feel-good film and that's how I felt wh...   \n",
       "7                   I couldn't take them seriously.     \n",
       "12  Not too screamy not to masculine but just righ...   \n",
       "21  This movie is excellent!Angel is beautiful and...   \n",
       "36          The soundtrack wasn't terrible, either.     \n",
       "\n",
       "                                               scores  compound_score  \\\n",
       "3   {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...          0.0000   \n",
       "7   {'neg': 0.0, 'neu': 0.664, 'pos': 0.336, 'comp...          0.1326   \n",
       "12  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...          0.0000   \n",
       "21  {'neg': 0.019, 'neu': 0.789, 'pos': 0.191, 'co...          0.9218   \n",
       "36  {'neg': 0.0, 'neu': 0.61, 'pos': 0.39, 'compou...          0.3724   \n",
       "\n",
       "    negative_score  positive_score  \\\n",
       "3            0.000           0.000   \n",
       "7            0.000           0.336   \n",
       "12           0.000           0.000   \n",
       "21           0.019           0.191   \n",
       "36           0.000           0.390   \n",
       "\n",
       "                                        processedText  topicmodel  Polarity  \\\n",
       "3                feelgood film thats felt came cinema           0         1   \n",
       "7                              couldnt take seriously           0         0   \n",
       "12                            screamy masculine right           3         1   \n",
       "21  movie excellentangel beautiful scamp adorableh...           0         1   \n",
       "36                   soundtrack wasnt terrible either           0         1   \n",
       "\n",
       "    Predicted  neutral_score  \n",
       "3           0          1.000  \n",
       "7           1          0.664  \n",
       "12          0          1.000  \n",
       "21          0          0.789  \n",
       "36          0          0.610  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mismatch.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For analysis purpose, I am also creating another dataframe which has the correct predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe with correct predicted values\n",
    "df_match = dffinal[dffinal[\"Polarity\"] == dffinal[\"Predicted\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets look at the statistics of the two dataframe (Match & MisMatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compound_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>topicmodel</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>neutral_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>485.000000</td>\n",
       "      <td>485.000000</td>\n",
       "      <td>485.000000</td>\n",
       "      <td>485.000000</td>\n",
       "      <td>485.000000</td>\n",
       "      <td>485.000000</td>\n",
       "      <td>485.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.105759</td>\n",
       "      <td>0.128705</td>\n",
       "      <td>0.186763</td>\n",
       "      <td>1.348454</td>\n",
       "      <td>0.476289</td>\n",
       "      <td>0.476289</td>\n",
       "      <td>0.684528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.533711</td>\n",
       "      <td>0.173385</td>\n",
       "      <td>0.200162</td>\n",
       "      <td>1.077677</td>\n",
       "      <td>0.499953</td>\n",
       "      <td>0.499953</td>\n",
       "      <td>0.192665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.908000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.361200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.571000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.024300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.697000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.599400</td>\n",
       "      <td>0.229000</td>\n",
       "      <td>0.286000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.943000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.889000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       compound_score  negative_score  positive_score  topicmodel    Polarity  \\\n",
       "count      485.000000      485.000000      485.000000  485.000000  485.000000   \n",
       "mean         0.105759        0.128705        0.186763    1.348454    0.476289   \n",
       "std          0.533711        0.173385        0.200162    1.077677    0.499953   \n",
       "min         -0.908000        0.000000        0.000000    0.000000    0.000000   \n",
       "25%         -0.361200        0.000000        0.000000    0.000000    0.000000   \n",
       "50%          0.024300        0.000000        0.156000    1.000000    0.000000   \n",
       "75%          0.599400        0.229000        0.286000    2.000000    1.000000   \n",
       "max          0.943000        1.000000        0.889000    3.000000    1.000000   \n",
       "\n",
       "        Predicted  neutral_score  \n",
       "count  485.000000     485.000000  \n",
       "mean     0.476289       0.684528  \n",
       "std      0.499953       0.192665  \n",
       "min      0.000000       0.000000  \n",
       "25%      0.000000       0.571000  \n",
       "50%      0.000000       0.697000  \n",
       "75%      1.000000       0.800000  \n",
       "max      1.000000       1.000000  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_match.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compound_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>topicmodel</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>neutral_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>115.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>115.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.106650</td>\n",
       "      <td>0.066270</td>\n",
       "      <td>0.124548</td>\n",
       "      <td>1.426087</td>\n",
       "      <td>0.713043</td>\n",
       "      <td>0.286957</td>\n",
       "      <td>0.809174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.349452</td>\n",
       "      <td>0.097416</td>\n",
       "      <td>0.129097</td>\n",
       "      <td>1.132153</td>\n",
       "      <td>0.454321</td>\n",
       "      <td>0.454321</td>\n",
       "      <td>0.161421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.746900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.351000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.061850</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.706000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.116000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.802000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.377100</td>\n",
       "      <td>0.121000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.921800</td>\n",
       "      <td>0.453000</td>\n",
       "      <td>0.528000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       compound_score  negative_score  positive_score  topicmodel    Polarity  \\\n",
       "count      115.000000      115.000000      115.000000  115.000000  115.000000   \n",
       "mean         0.106650        0.066270        0.124548    1.426087    0.713043   \n",
       "std          0.349452        0.097416        0.129097    1.132153    0.454321   \n",
       "min         -0.746900        0.000000        0.000000    0.000000    0.000000   \n",
       "25%         -0.061850        0.000000        0.000000    0.000000    0.000000   \n",
       "50%          0.000000        0.000000        0.116000    1.000000    1.000000   \n",
       "75%          0.377100        0.121000        0.200000    2.000000    1.000000   \n",
       "max          0.921800        0.453000        0.528000    3.000000    1.000000   \n",
       "\n",
       "        Predicted  neutral_score  \n",
       "count  115.000000     115.000000  \n",
       "mean     0.286957       0.809174  \n",
       "std      0.454321       0.161421  \n",
       "min      0.000000       0.351000  \n",
       "25%      0.000000       0.706000  \n",
       "50%      0.000000       0.802000  \n",
       "75%      1.000000       1.000000  \n",
       "max      1.000000       1.000000  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mismatch.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Neutral Score is higher in the mismatched dataframe\n",
    "When we look at the statistics value of both the dataframe, we see that the mean neutral score in the mismatched dataframe is higher compared to the mean neutral score in the matched dataframe. It can be inferred that majority of the reviews that were incorrectly predicted were neutral in their nature, that is to say, those reviews were neither bad or good. To dig deeper, we will look at the count of top 30 words in every category and will compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-99-966fc98f1b13>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_match['temp_list'] = df_match['processedText'].apply(lambda x:str(x).split())\n",
      "<ipython-input-99-966fc98f1b13>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_mismatch['temp_list'] = df_mismatch['processedText'].apply(lambda x:str(x).split())\n"
     ]
    }
   ],
   "source": [
    "df_match['temp_list'] = df_match['processedText'].apply(lambda x:str(x).split())\n",
    "df_mismatch['temp_list'] = df_mismatch['processedText'].apply(lambda x:str(x).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>scores</th>\n",
       "      <th>compound_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>processedText</th>\n",
       "      <th>topicmodel</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>neutral_score</th>\n",
       "      <th>temp_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A good commentary of today's love and undoubte...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.438, 'pos': 0.562, 'comp...</td>\n",
       "      <td>0.8402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.562</td>\n",
       "      <td>good commentary today love undoubtedly film wo...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.438</td>\n",
       "      <td>[good, commentary, today, love, undoubtedly, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For people who are first timers in film making...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'comp...</td>\n",
       "      <td>0.6467</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.234</td>\n",
       "      <td>people first timer film making think excellent...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.766</td>\n",
       "      <td>[people, first, timer, film, making, think, ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It was very popular when I was in the cinema, ...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.554, 'pos': 0.446, 'comp...</td>\n",
       "      <td>0.9020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.446</td>\n",
       "      <td>popular cinema good house good reaction plenty...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.554</td>\n",
       "      <td>[popular, cinema, good, house, good, reaction,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It has northern humour and positive about the ...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.573, 'pos': 0.427, 'comp...</td>\n",
       "      <td>0.7717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.427</td>\n",
       "      <td>northern humour positive community represents</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.573</td>\n",
       "      <td>[northern, humour, positive, community, repres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I rather enjoyed it.</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.377, 'pos': 0.623, 'comp...</td>\n",
       "      <td>0.5106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.623</td>\n",
       "      <td>rather enjoyed</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.377</td>\n",
       "      <td>[rather, enjoyed]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0  A good commentary of today's love and undoubte...   \n",
       "1  For people who are first timers in film making...   \n",
       "2  It was very popular when I was in the cinema, ...   \n",
       "4  It has northern humour and positive about the ...   \n",
       "5                             I rather enjoyed it.     \n",
       "\n",
       "                                              scores  compound_score  \\\n",
       "0  {'neg': 0.0, 'neu': 0.438, 'pos': 0.562, 'comp...          0.8402   \n",
       "1  {'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'comp...          0.6467   \n",
       "2  {'neg': 0.0, 'neu': 0.554, 'pos': 0.446, 'comp...          0.9020   \n",
       "4  {'neg': 0.0, 'neu': 0.573, 'pos': 0.427, 'comp...          0.7717   \n",
       "5  {'neg': 0.0, 'neu': 0.377, 'pos': 0.623, 'comp...          0.5106   \n",
       "\n",
       "   negative_score  positive_score  \\\n",
       "0             0.0           0.562   \n",
       "1             0.0           0.234   \n",
       "2             0.0           0.446   \n",
       "4             0.0           0.427   \n",
       "5             0.0           0.623   \n",
       "\n",
       "                                       processedText  topicmodel  Polarity  \\\n",
       "0  good commentary today love undoubtedly film wo...           2         1   \n",
       "1  people first timer film making think excellent...           1         1   \n",
       "2  popular cinema good house good reaction plenty...           0         1   \n",
       "4      northern humour positive community represents           3         1   \n",
       "5                                     rather enjoyed           1         1   \n",
       "\n",
       "   Predicted  neutral_score                                          temp_list  \n",
       "0          1          0.438  [good, commentary, today, love, undoubtedly, f...  \n",
       "1          1          0.766  [people, first, timer, film, making, think, ex...  \n",
       "2          1          0.554  [popular, cinema, good, house, good, reaction,...  \n",
       "4          1          0.573  [northern, humour, positive, community, repres...  \n",
       "5          1          0.377                                  [rather, enjoyed]  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_match.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe of all those instances with Polarity = 1 and were correctly predicted\n",
    "#We will check the word frequency in this dataframe\n",
    "positive_match = df_match[df_match[\"Polarity\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>scores</th>\n",
       "      <th>compound_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>processedText</th>\n",
       "      <th>topicmodel</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>neutral_score</th>\n",
       "      <th>temp_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A good commentary of today's love and undoubte...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.438, 'pos': 0.562, 'comp...</td>\n",
       "      <td>0.8402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.562</td>\n",
       "      <td>good commentary today love undoubtedly film wo...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.438</td>\n",
       "      <td>[good, commentary, today, love, undoubtedly, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For people who are first timers in film making...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'comp...</td>\n",
       "      <td>0.6467</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.234</td>\n",
       "      <td>people first timer film making think excellent...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.766</td>\n",
       "      <td>[people, first, timer, film, making, think, ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It was very popular when I was in the cinema, ...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.554, 'pos': 0.446, 'comp...</td>\n",
       "      <td>0.9020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.446</td>\n",
       "      <td>popular cinema good house good reaction plenty...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.554</td>\n",
       "      <td>[popular, cinema, good, house, good, reaction,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It has northern humour and positive about the ...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.573, 'pos': 0.427, 'comp...</td>\n",
       "      <td>0.7717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.427</td>\n",
       "      <td>northern humour positive community represents</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.573</td>\n",
       "      <td>[northern, humour, positive, community, repres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I rather enjoyed it.</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.377, 'pos': 0.623, 'comp...</td>\n",
       "      <td>0.5106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.623</td>\n",
       "      <td>rather enjoyed</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.377</td>\n",
       "      <td>[rather, enjoyed]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0  A good commentary of today's love and undoubte...   \n",
       "1  For people who are first timers in film making...   \n",
       "2  It was very popular when I was in the cinema, ...   \n",
       "4  It has northern humour and positive about the ...   \n",
       "5                             I rather enjoyed it.     \n",
       "\n",
       "                                              scores  compound_score  \\\n",
       "0  {'neg': 0.0, 'neu': 0.438, 'pos': 0.562, 'comp...          0.8402   \n",
       "1  {'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'comp...          0.6467   \n",
       "2  {'neg': 0.0, 'neu': 0.554, 'pos': 0.446, 'comp...          0.9020   \n",
       "4  {'neg': 0.0, 'neu': 0.573, 'pos': 0.427, 'comp...          0.7717   \n",
       "5  {'neg': 0.0, 'neu': 0.377, 'pos': 0.623, 'comp...          0.5106   \n",
       "\n",
       "   negative_score  positive_score  \\\n",
       "0             0.0           0.562   \n",
       "1             0.0           0.234   \n",
       "2             0.0           0.446   \n",
       "4             0.0           0.427   \n",
       "5             0.0           0.623   \n",
       "\n",
       "                                       processedText  topicmodel  Polarity  \\\n",
       "0  good commentary today love undoubtedly film wo...           2         1   \n",
       "1  people first timer film making think excellent...           1         1   \n",
       "2  popular cinema good house good reaction plenty...           0         1   \n",
       "4      northern humour positive community represents           3         1   \n",
       "5                                     rather enjoyed           1         1   \n",
       "\n",
       "   Predicted  neutral_score                                          temp_list  \n",
       "0          1          0.438  [good, commentary, today, love, undoubtedly, f...  \n",
       "1          1          0.766  [people, first, timer, film, making, think, ex...  \n",
       "2          1          0.554  [popular, cinema, good, house, good, reaction,...  \n",
       "4          1          0.573  [northern, humour, positive, community, repres...  \n",
       "5          1          0.377                                  [rather, enjoyed]  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_match.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_6efd6_row0_col1{\n",
       "            background-color:  #08306b;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_6efd6_row1_col1{\n",
       "            background-color:  #3585bf;\n",
       "            color:  #000000;\n",
       "        }#T_6efd6_row2_col1{\n",
       "            background-color:  #aed1e7;\n",
       "            color:  #000000;\n",
       "        }#T_6efd6_row3_col1{\n",
       "            background-color:  #b9d6ea;\n",
       "            color:  #000000;\n",
       "        }#T_6efd6_row4_col1{\n",
       "            background-color:  #c9ddf0;\n",
       "            color:  #000000;\n",
       "        }#T_6efd6_row5_col1{\n",
       "            background-color:  #d7e6f5;\n",
       "            color:  #000000;\n",
       "        }#T_6efd6_row6_col1{\n",
       "            background-color:  #deebf7;\n",
       "            color:  #000000;\n",
       "        }#T_6efd6_row7_col1{\n",
       "            background-color:  #e5eff9;\n",
       "            color:  #000000;\n",
       "        }#T_6efd6_row8_col1,#T_6efd6_row9_col1{\n",
       "            background-color:  #e9f2fa;\n",
       "            color:  #000000;\n",
       "        }#T_6efd6_row10_col1,#T_6efd6_row11_col1,#T_6efd6_row12_col1{\n",
       "            background-color:  #edf4fc;\n",
       "            color:  #000000;\n",
       "        }#T_6efd6_row13_col1,#T_6efd6_row14_col1,#T_6efd6_row15_col1,#T_6efd6_row16_col1{\n",
       "            background-color:  #f0f6fd;\n",
       "            color:  #000000;\n",
       "        }#T_6efd6_row17_col1,#T_6efd6_row18_col1,#T_6efd6_row19_col1,#T_6efd6_row20_col1,#T_6efd6_row21_col1,#T_6efd6_row22_col1,#T_6efd6_row23_col1{\n",
       "            background-color:  #f4f9fe;\n",
       "            color:  #000000;\n",
       "        }#T_6efd6_row24_col1,#T_6efd6_row25_col1,#T_6efd6_row26_col1,#T_6efd6_row27_col1,#T_6efd6_row28_col1,#T_6efd6_row29_col1{\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_6efd6_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Common_words</th>        <th class=\"col_heading level0 col1\" >count</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_6efd6_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_6efd6_row0_col0\" class=\"data row0 col0\" >film</td>\n",
       "                        <td id=\"T_6efd6_row0_col1\" class=\"data row0 col1\" >62</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6efd6_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_6efd6_row1_col0\" class=\"data row1 col0\" >movie</td>\n",
       "                        <td id=\"T_6efd6_row1_col1\" class=\"data row1 col1\" >44</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6efd6_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_6efd6_row2_col0\" class=\"data row2 col0\" >good</td>\n",
       "                        <td id=\"T_6efd6_row2_col1\" class=\"data row2 col1\" >25</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6efd6_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_6efd6_row3_col0\" class=\"data row3 col0\" >great</td>\n",
       "                        <td id=\"T_6efd6_row3_col1\" class=\"data row3 col1\" >23</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6efd6_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_6efd6_row4_col0\" class=\"data row4 col0\" >character</td>\n",
       "                        <td id=\"T_6efd6_row4_col1\" class=\"data row4 col1\" >20</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6efd6_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_6efd6_row5_col0\" class=\"data row5 col0\" >one</td>\n",
       "                        <td id=\"T_6efd6_row5_col1\" class=\"data row5 col1\" >16</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6efd6_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_6efd6_row6_col0\" class=\"data row6 col0\" >wonderful</td>\n",
       "                        <td id=\"T_6efd6_row6_col1\" class=\"data row6 col1\" >14</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6efd6_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_6efd6_row7_col0\" class=\"data row7 col0\" >actor</td>\n",
       "                        <td id=\"T_6efd6_row7_col1\" class=\"data row7 col1\" >12</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6efd6_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_6efd6_row8_col0\" class=\"data row8 col0\" >like</td>\n",
       "                        <td id=\"T_6efd6_row8_col1\" class=\"data row8 col1\" >11</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6efd6_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_6efd6_row9_col0\" class=\"data row9 col0\" >acting</td>\n",
       "                        <td id=\"T_6efd6_row9_col1\" class=\"data row9 col1\" >11</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6efd6_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "                        <td id=\"T_6efd6_row10_col0\" class=\"data row10 col0\" >love</td>\n",
       "                        <td id=\"T_6efd6_row10_col1\" class=\"data row10 col1\" >10</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6efd6_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "                        <td id=\"T_6efd6_row11_col0\" class=\"data row11 col0\" >well</td>\n",
       "                        <td id=\"T_6efd6_row11_col1\" class=\"data row11 col1\" >10</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6efd6_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "                        <td id=\"T_6efd6_row12_col0\" class=\"data row12 col0\" >cast</td>\n",
       "                        <td id=\"T_6efd6_row12_col1\" class=\"data row12 col1\" >10</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6efd6_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "                        <td id=\"T_6efd6_row13_col0\" class=\"data row13 col0\" >excellent</td>\n",
       "                        <td id=\"T_6efd6_row13_col1\" class=\"data row13 col1\" >9</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6efd6_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "                        <td id=\"T_6efd6_row14_col0\" class=\"data row14 col0\" >performance</td>\n",
       "                        <td id=\"T_6efd6_row14_col1\" class=\"data row14 col1\" >9</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6efd6_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "                        <td id=\"T_6efd6_row15_col0\" class=\"data row15 col0\" >also</td>\n",
       "                        <td id=\"T_6efd6_row15_col1\" class=\"data row15 col1\" >9</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6efd6_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "                        <td id=\"T_6efd6_row16_col0\" class=\"data row16 col0\" >story</td>\n",
       "                        <td id=\"T_6efd6_row16_col1\" class=\"data row16 col1\" >9</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6efd6_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "                        <td id=\"T_6efd6_row17_col0\" class=\"data row17 col0\" >people</td>\n",
       "                        <td id=\"T_6efd6_row17_col1\" class=\"data row17 col1\" >8</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6efd6_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "                        <td id=\"T_6efd6_row18_col0\" class=\"data row18 col0\" >job</td>\n",
       "                        <td id=\"T_6efd6_row18_col1\" class=\"data row18 col1\" >8</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6efd6_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "                        <td id=\"T_6efd6_row19_col0\" class=\"data row19 col0\" >time</td>\n",
       "                        <td id=\"T_6efd6_row19_col1\" class=\"data row19 col1\" >8</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6efd6_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "                        <td id=\"T_6efd6_row20_col0\" class=\"data row20 col0\" >best</td>\n",
       "                        <td id=\"T_6efd6_row20_col1\" class=\"data row20 col1\" >8</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6efd6_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "                        <td id=\"T_6efd6_row21_col0\" class=\"data row21 col0\" >year</td>\n",
       "                        <td id=\"T_6efd6_row21_col1\" class=\"data row21 col1\" >8</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6efd6_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "                        <td id=\"T_6efd6_row22_col0\" class=\"data row22 col0\" >give</td>\n",
       "                        <td id=\"T_6efd6_row22_col1\" class=\"data row22 col1\" >8</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6efd6_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "                        <td id=\"T_6efd6_row23_col0\" class=\"data row23 col0\" >still</td>\n",
       "                        <td id=\"T_6efd6_row23_col1\" class=\"data row23 col1\" >8</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6efd6_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "                        <td id=\"T_6efd6_row24_col0\" class=\"data row24 col0\" >think</td>\n",
       "                        <td id=\"T_6efd6_row24_col1\" class=\"data row24 col1\" >7</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6efd6_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "                        <td id=\"T_6efd6_row25_col0\" class=\"data row25 col0\" >enjoyed</td>\n",
       "                        <td id=\"T_6efd6_row25_col1\" class=\"data row25 col1\" >7</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6efd6_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "                        <td id=\"T_6efd6_row26_col0\" class=\"data row26 col0\" >liked</td>\n",
       "                        <td id=\"T_6efd6_row26_col1\" class=\"data row26 col1\" >7</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6efd6_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "                        <td id=\"T_6efd6_row27_col0\" class=\"data row27 col0\" >really</td>\n",
       "                        <td id=\"T_6efd6_row27_col1\" class=\"data row27 col1\" >7</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6efd6_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "                        <td id=\"T_6efd6_row28_col0\" class=\"data row28 col0\" >funny</td>\n",
       "                        <td id=\"T_6efd6_row28_col1\" class=\"data row28 col1\" >7</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6efd6_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "                        <td id=\"T_6efd6_row29_col0\" class=\"data row29 col0\" >see</td>\n",
       "                        <td id=\"T_6efd6_row29_col1\" class=\"data row29 col1\" >7</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1dc37e11160>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating top 30 word by counts for positive reviews that were correctly predicted\n",
    "top_pos = Counter([item for sublist in positive_match['temp_list'] for item in sublist])\n",
    "temp_matchpos = pd.DataFrame(top_pos.most_common(30))\n",
    "temp_matchpos.columns = ['Common_words','count']\n",
    "temp_matchpos.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe of all those instances with Polarity = o and were correctly predicted\n",
    "#We will check the word frequency in this dataframe\n",
    "negative_match = df_match[df_match[\"Polarity\"]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>scores</th>\n",
       "      <th>compound_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>processedText</th>\n",
       "      <th>topicmodel</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>neutral_score</th>\n",
       "      <th>temp_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I am a fan of his ... This movie sucked really...</td>\n",
       "      <td>{'neg': 0.422, 'neu': 0.435, 'pos': 0.143, 'co...</td>\n",
       "      <td>-0.6697</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.143</td>\n",
       "      <td>fan movie sucked really bad</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.435</td>\n",
       "      <td>[fan, movie, sucked, really, bad]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Even worse than Ticker!</td>\n",
       "      <td>{'neg': 0.531, 'neu': 0.469, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.5255</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.000</td>\n",
       "      <td>even worse ticker</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>[even, worse, ticker]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>&amp; That movie was bad.</td>\n",
       "      <td>{'neg': 0.538, 'neu': 0.462, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.5423</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.000</td>\n",
       "      <td>movie bad</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.462</td>\n",
       "      <td>[movie, bad]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Only like 3 or 4 buildings used, a couple of l...</td>\n",
       "      <td>{'neg': 0.228, 'neu': 0.604, 'pos': 0.168, 'co...</td>\n",
       "      <td>-0.2244</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.168</td>\n",
       "      <td>like building used couple location maybe poor ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.604</td>\n",
       "      <td>[like, building, used, couple, location, maybe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>It just blew.</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>blew</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>[blew]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Sentence  \\\n",
       "16  I am a fan of his ... This movie sucked really...   \n",
       "17                          Even worse than Ticker!     \n",
       "18                            & That movie was bad.     \n",
       "19  Only like 3 or 4 buildings used, a couple of l...   \n",
       "20                                    It just blew.     \n",
       "\n",
       "                                               scores  compound_score  \\\n",
       "16  {'neg': 0.422, 'neu': 0.435, 'pos': 0.143, 'co...         -0.6697   \n",
       "17  {'neg': 0.531, 'neu': 0.469, 'pos': 0.0, 'comp...         -0.5255   \n",
       "18  {'neg': 0.538, 'neu': 0.462, 'pos': 0.0, 'comp...         -0.5423   \n",
       "19  {'neg': 0.228, 'neu': 0.604, 'pos': 0.168, 'co...         -0.2244   \n",
       "20  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...          0.0000   \n",
       "\n",
       "    negative_score  positive_score  \\\n",
       "16           0.422           0.143   \n",
       "17           0.531           0.000   \n",
       "18           0.538           0.000   \n",
       "19           0.228           0.168   \n",
       "20           0.000           0.000   \n",
       "\n",
       "                                        processedText  topicmodel  Polarity  \\\n",
       "16                        fan movie sucked really bad           0         0   \n",
       "17                                  even worse ticker           0         0   \n",
       "18                                          movie bad           3         0   \n",
       "19  like building used couple location maybe poor ...           2         0   \n",
       "20                                               blew           3         0   \n",
       "\n",
       "    Predicted  neutral_score  \\\n",
       "16          0          0.435   \n",
       "17          0          0.469   \n",
       "18          0          0.462   \n",
       "19          0          0.604   \n",
       "20          0          1.000   \n",
       "\n",
       "                                            temp_list  \n",
       "16                  [fan, movie, sucked, really, bad]  \n",
       "17                              [even, worse, ticker]  \n",
       "18                                       [movie, bad]  \n",
       "19  [like, building, used, couple, location, maybe...  \n",
       "20                                             [blew]  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_match.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_8d2c1_row0_col1{\n",
       "            background-color:  #08306b;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_8d2c1_row1_col1{\n",
       "            background-color:  #125ea6;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_8d2c1_row2_col1{\n",
       "            background-color:  #2575b7;\n",
       "            color:  #000000;\n",
       "        }#T_8d2c1_row3_col1{\n",
       "            background-color:  #94c4df;\n",
       "            color:  #000000;\n",
       "        }#T_8d2c1_row4_col1{\n",
       "            background-color:  #c7dcef;\n",
       "            color:  #000000;\n",
       "        }#T_8d2c1_row5_col1{\n",
       "            background-color:  #ccdff1;\n",
       "            color:  #000000;\n",
       "        }#T_8d2c1_row6_col1,#T_8d2c1_row7_col1{\n",
       "            background-color:  #d0e1f2;\n",
       "            color:  #000000;\n",
       "        }#T_8d2c1_row8_col1{\n",
       "            background-color:  #d4e4f4;\n",
       "            color:  #000000;\n",
       "        }#T_8d2c1_row9_col1{\n",
       "            background-color:  #dceaf6;\n",
       "            color:  #000000;\n",
       "        }#T_8d2c1_row10_col1,#T_8d2c1_row11_col1,#T_8d2c1_row12_col1{\n",
       "            background-color:  #e1edf8;\n",
       "            color:  #000000;\n",
       "        }#T_8d2c1_row13_col1{\n",
       "            background-color:  #e6f0f9;\n",
       "            color:  #000000;\n",
       "        }#T_8d2c1_row14_col1,#T_8d2c1_row15_col1,#T_8d2c1_row16_col1{\n",
       "            background-color:  #eaf2fb;\n",
       "            color:  #000000;\n",
       "        }#T_8d2c1_row17_col1,#T_8d2c1_row18_col1,#T_8d2c1_row19_col1,#T_8d2c1_row20_col1,#T_8d2c1_row21_col1,#T_8d2c1_row22_col1,#T_8d2c1_row23_col1{\n",
       "            background-color:  #eef5fc;\n",
       "            color:  #000000;\n",
       "        }#T_8d2c1_row24_col1,#T_8d2c1_row25_col1,#T_8d2c1_row26_col1{\n",
       "            background-color:  #f3f8fe;\n",
       "            color:  #000000;\n",
       "        }#T_8d2c1_row27_col1,#T_8d2c1_row28_col1,#T_8d2c1_row29_col1{\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_8d2c1_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Common_words</th>        <th class=\"col_heading level0 col1\" >count</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_8d2c1_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_8d2c1_row0_col0\" class=\"data row0 col0\" >movie</td>\n",
       "                        <td id=\"T_8d2c1_row0_col1\" class=\"data row0 col1\" >52</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8d2c1_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_8d2c1_row1_col0\" class=\"data row1 col0\" >bad</td>\n",
       "                        <td id=\"T_8d2c1_row1_col1\" class=\"data row1 col1\" >44</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8d2c1_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_8d2c1_row2_col0\" class=\"data row2 col0\" >film</td>\n",
       "                        <td id=\"T_8d2c1_row2_col1\" class=\"data row2 col1\" >40</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8d2c1_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_8d2c1_row3_col0\" class=\"data row3 col0\" >one</td>\n",
       "                        <td id=\"T_8d2c1_row3_col1\" class=\"data row3 col1\" >25</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8d2c1_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_8d2c1_row4_col0\" class=\"data row4 col0\" >time</td>\n",
       "                        <td id=\"T_8d2c1_row4_col1\" class=\"data row4 col1\" >18</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8d2c1_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_8d2c1_row5_col0\" class=\"data row5 col0\" >even</td>\n",
       "                        <td id=\"T_8d2c1_row5_col1\" class=\"data row5 col1\" >17</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8d2c1_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_8d2c1_row6_col0\" class=\"data row6 col0\" >like</td>\n",
       "                        <td id=\"T_8d2c1_row6_col1\" class=\"data row6 col1\" >16</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8d2c1_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_8d2c1_row7_col0\" class=\"data row7 col0\" >acting</td>\n",
       "                        <td id=\"T_8d2c1_row7_col1\" class=\"data row7 col1\" >16</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8d2c1_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_8d2c1_row8_col0\" class=\"data row8 col0\" >script</td>\n",
       "                        <td id=\"T_8d2c1_row8_col1\" class=\"data row8 col1\" >15</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8d2c1_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_8d2c1_row9_col0\" class=\"data row9 col0\" >plot</td>\n",
       "                        <td id=\"T_8d2c1_row9_col1\" class=\"data row9 col1\" >13</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8d2c1_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "                        <td id=\"T_8d2c1_row10_col0\" class=\"data row10 col0\" >really</td>\n",
       "                        <td id=\"T_8d2c1_row10_col1\" class=\"data row10 col1\" >12</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8d2c1_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "                        <td id=\"T_8d2c1_row11_col0\" class=\"data row11 col0\" >show</td>\n",
       "                        <td id=\"T_8d2c1_row11_col1\" class=\"data row11 col1\" >12</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8d2c1_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "                        <td id=\"T_8d2c1_row12_col0\" class=\"data row12 col0\" >dont</td>\n",
       "                        <td id=\"T_8d2c1_row12_col1\" class=\"data row12 col1\" >12</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8d2c1_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "                        <td id=\"T_8d2c1_row13_col0\" class=\"data row13 col0\" >ever</td>\n",
       "                        <td id=\"T_8d2c1_row13_col1\" class=\"data row13 col1\" >11</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8d2c1_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "                        <td id=\"T_8d2c1_row14_col0\" class=\"data row14 col0\" >look</td>\n",
       "                        <td id=\"T_8d2c1_row14_col1\" class=\"data row14 col1\" >10</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8d2c1_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "                        <td id=\"T_8d2c1_row15_col0\" class=\"data row15 col0\" >thing</td>\n",
       "                        <td id=\"T_8d2c1_row15_col1\" class=\"data row15 col1\" >10</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8d2c1_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "                        <td id=\"T_8d2c1_row16_col0\" class=\"data row16 col0\" >awful</td>\n",
       "                        <td id=\"T_8d2c1_row16_col1\" class=\"data row16 col1\" >10</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8d2c1_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "                        <td id=\"T_8d2c1_row17_col0\" class=\"data row17 col0\" >scene</td>\n",
       "                        <td id=\"T_8d2c1_row17_col1\" class=\"data row17 col1\" >9</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8d2c1_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "                        <td id=\"T_8d2c1_row18_col0\" class=\"data row18 col0\" >would</td>\n",
       "                        <td id=\"T_8d2c1_row18_col1\" class=\"data row18 col1\" >9</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8d2c1_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "                        <td id=\"T_8d2c1_row19_col0\" class=\"data row19 col0\" >waste</td>\n",
       "                        <td id=\"T_8d2c1_row19_col1\" class=\"data row19 col1\" >9</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8d2c1_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "                        <td id=\"T_8d2c1_row20_col0\" class=\"data row20 col0\" >real</td>\n",
       "                        <td id=\"T_8d2c1_row20_col1\" class=\"data row20 col1\" >9</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8d2c1_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "                        <td id=\"T_8d2c1_row21_col0\" class=\"data row21 col0\" >seen</td>\n",
       "                        <td id=\"T_8d2c1_row21_col1\" class=\"data row21 col1\" >9</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8d2c1_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "                        <td id=\"T_8d2c1_row22_col0\" class=\"data row22 col0\" >story</td>\n",
       "                        <td id=\"T_8d2c1_row22_col1\" class=\"data row22 col1\" >9</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8d2c1_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "                        <td id=\"T_8d2c1_row23_col0\" class=\"data row23 col0\" >suck</td>\n",
       "                        <td id=\"T_8d2c1_row23_col1\" class=\"data row23 col1\" >9</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8d2c1_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "                        <td id=\"T_8d2c1_row24_col0\" class=\"data row24 col0\" >made</td>\n",
       "                        <td id=\"T_8d2c1_row24_col1\" class=\"data row24 col1\" >8</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8d2c1_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "                        <td id=\"T_8d2c1_row25_col0\" class=\"data row25 col0\" >line</td>\n",
       "                        <td id=\"T_8d2c1_row25_col1\" class=\"data row25 col1\" >8</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8d2c1_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "                        <td id=\"T_8d2c1_row26_col0\" class=\"data row26 col0\" >little</td>\n",
       "                        <td id=\"T_8d2c1_row26_col1\" class=\"data row26 col1\" >8</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8d2c1_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "                        <td id=\"T_8d2c1_row27_col0\" class=\"data row27 col0\" >worse</td>\n",
       "                        <td id=\"T_8d2c1_row27_col1\" class=\"data row27 col1\" >7</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8d2c1_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "                        <td id=\"T_8d2c1_row28_col0\" class=\"data row28 col0\" >could</td>\n",
       "                        <td id=\"T_8d2c1_row28_col1\" class=\"data row28 col1\" >7</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8d2c1_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "                        <td id=\"T_8d2c1_row29_col0\" class=\"data row29 col0\" >see</td>\n",
       "                        <td id=\"T_8d2c1_row29_col1\" class=\"data row29 col1\" >7</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1dc37f1d310>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating top words by counts for negative reviews that were correctly predicted\n",
    "top_negmatch = Counter([item for sublist in negative_match['temp_list'] for item in sublist])\n",
    "temp_matchneg = pd.DataFrame(top_negmatch.most_common(30))\n",
    "temp_matchneg.columns = ['Common_words','count']\n",
    "temp_matchneg.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets check for the word counts in the dataset where predictions were incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe of all those instances with Polarity = 1 but were incorrectly predicted\n",
    "#We will check the word frequency in this dataframe\n",
    "positive_mismatch = df_mismatch[df_mismatch[\"Polarity\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>scores</th>\n",
       "      <th>compound_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>processedText</th>\n",
       "      <th>topicmodel</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>neutral_score</th>\n",
       "      <th>temp_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's a feel-good film and that's how I felt wh...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>feelgood film thats felt came cinema</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>[feelgood, film, thats, felt, came, cinema]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Not too screamy not to masculine but just righ...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>screamy masculine right</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>[screamy, masculine, right]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>This movie is excellent!Angel is beautiful and...</td>\n",
       "      <td>{'neg': 0.019, 'neu': 0.789, 'pos': 0.191, 'co...</td>\n",
       "      <td>0.9218</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.191</td>\n",
       "      <td>movie excellentangel beautiful scamp adorableh...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.789</td>\n",
       "      <td>[movie, excellentangel, beautiful, scamp, ador...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>The soundtrack wasn't terrible, either.</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.61, 'pos': 0.39, 'compou...</td>\n",
       "      <td>0.3724</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.390</td>\n",
       "      <td>soundtrack wasnt terrible either</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.610</td>\n",
       "      <td>[soundtrack, wasnt, terrible, either]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Still, it was the SETS that got a big \"10\" on ...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>still set got big oyvey scale</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>[still, set, got, big, oyvey, scale]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Sentence  \\\n",
       "3   It's a feel-good film and that's how I felt wh...   \n",
       "12  Not too screamy not to masculine but just righ...   \n",
       "21  This movie is excellent!Angel is beautiful and...   \n",
       "36          The soundtrack wasn't terrible, either.     \n",
       "38  Still, it was the SETS that got a big \"10\" on ...   \n",
       "\n",
       "                                               scores  compound_score  \\\n",
       "3   {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...          0.0000   \n",
       "12  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...          0.0000   \n",
       "21  {'neg': 0.019, 'neu': 0.789, 'pos': 0.191, 'co...          0.9218   \n",
       "36  {'neg': 0.0, 'neu': 0.61, 'pos': 0.39, 'compou...          0.3724   \n",
       "38  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...          0.0000   \n",
       "\n",
       "    negative_score  positive_score  \\\n",
       "3            0.000           0.000   \n",
       "12           0.000           0.000   \n",
       "21           0.019           0.191   \n",
       "36           0.000           0.390   \n",
       "38           0.000           0.000   \n",
       "\n",
       "                                        processedText  topicmodel  Polarity  \\\n",
       "3                feelgood film thats felt came cinema           0         1   \n",
       "12                            screamy masculine right           3         1   \n",
       "21  movie excellentangel beautiful scamp adorableh...           0         1   \n",
       "36                   soundtrack wasnt terrible either           0         1   \n",
       "38                      still set got big oyvey scale           1         1   \n",
       "\n",
       "    Predicted  neutral_score  \\\n",
       "3           0          1.000   \n",
       "12          0          1.000   \n",
       "21          0          0.789   \n",
       "36          0          0.610   \n",
       "38          0          1.000   \n",
       "\n",
       "                                            temp_list  \n",
       "3         [feelgood, film, thats, felt, came, cinema]  \n",
       "12                        [screamy, masculine, right]  \n",
       "21  [movie, excellentangel, beautiful, scamp, ador...  \n",
       "36              [soundtrack, wasnt, terrible, either]  \n",
       "38               [still, set, got, big, oyvey, scale]  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_mismatch.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 30 words in the incorrect prediction dataset with actual polarity 1 are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_e2493_row0_col1{\n",
       "            background-color:  #7f0000;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_e2493_row1_col1{\n",
       "            background-color:  #c2140d;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_e2493_row2_col1{\n",
       "            background-color:  #fdad77;\n",
       "            color:  #000000;\n",
       "        }#T_e2493_row3_col1{\n",
       "            background-color:  #fdbf89;\n",
       "            color:  #000000;\n",
       "        }#T_e2493_row4_col1,#T_e2493_row5_col1{\n",
       "            background-color:  #fdd6a3;\n",
       "            color:  #000000;\n",
       "        }#T_e2493_row6_col1,#T_e2493_row7_col1,#T_e2493_row8_col1,#T_e2493_row9_col1{\n",
       "            background-color:  #fee0b7;\n",
       "            color:  #000000;\n",
       "        }#T_e2493_row10_col1,#T_e2493_row11_col1,#T_e2493_row12_col1,#T_e2493_row13_col1,#T_e2493_row14_col1,#T_e2493_row15_col1{\n",
       "            background-color:  #fee9ca;\n",
       "            color:  #000000;\n",
       "        }#T_e2493_row16_col1,#T_e2493_row17_col1,#T_e2493_row18_col1,#T_e2493_row19_col1,#T_e2493_row20_col1,#T_e2493_row21_col1,#T_e2493_row22_col1,#T_e2493_row23_col1,#T_e2493_row24_col1,#T_e2493_row25_col1,#T_e2493_row26_col1{\n",
       "            background-color:  #fff0db;\n",
       "            color:  #000000;\n",
       "        }#T_e2493_row27_col1,#T_e2493_row28_col1,#T_e2493_row29_col1{\n",
       "            background-color:  #fff7ec;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_e2493_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Common_words</th>        <th class=\"col_heading level0 col1\" >count</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_e2493_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_e2493_row0_col0\" class=\"data row0 col0\" >movie</td>\n",
       "                        <td id=\"T_e2493_row0_col1\" class=\"data row0 col1\" >19</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e2493_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_e2493_row1_col0\" class=\"data row1 col0\" >film</td>\n",
       "                        <td id=\"T_e2493_row1_col1\" class=\"data row1 col1\" >16</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e2493_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_e2493_row2_col0\" class=\"data row2 col0\" >one</td>\n",
       "                        <td id=\"T_e2493_row2_col1\" class=\"data row2 col1\" >9</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e2493_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_e2493_row3_col0\" class=\"data row3 col0\" >scene</td>\n",
       "                        <td id=\"T_e2493_row3_col1\" class=\"data row3 col1\" >8</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e2493_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_e2493_row4_col0\" class=\"data row4 col0\" >time</td>\n",
       "                        <td id=\"T_e2493_row4_col1\" class=\"data row4 col1\" >6</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e2493_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_e2493_row5_col0\" class=\"data row5 col0\" >character</td>\n",
       "                        <td id=\"T_e2493_row5_col1\" class=\"data row5 col1\" >6</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e2493_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_e2493_row6_col0\" class=\"data row6 col0\" >dont</td>\n",
       "                        <td id=\"T_e2493_row6_col1\" class=\"data row6 col1\" >5</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e2493_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_e2493_row7_col0\" class=\"data row7 col0\" >like</td>\n",
       "                        <td id=\"T_e2493_row7_col1\" class=\"data row7 col1\" >5</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e2493_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_e2493_row8_col0\" class=\"data row8 col0\" >see</td>\n",
       "                        <td id=\"T_e2493_row8_col1\" class=\"data row8 col1\" >5</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e2493_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_e2493_row9_col0\" class=\"data row9 col0\" >go</td>\n",
       "                        <td id=\"T_e2493_row9_col1\" class=\"data row9 col1\" >5</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e2493_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "                        <td id=\"T_e2493_row10_col0\" class=\"data row10 col0\" >never</td>\n",
       "                        <td id=\"T_e2493_row10_col1\" class=\"data row10 col1\" >4</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e2493_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "                        <td id=\"T_e2493_row11_col0\" class=\"data row11 col0\" >well</td>\n",
       "                        <td id=\"T_e2493_row11_col1\" class=\"data row11 col1\" >4</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e2493_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "                        <td id=\"T_e2493_row12_col0\" class=\"data row12 col0\" >want</td>\n",
       "                        <td id=\"T_e2493_row12_col1\" class=\"data row12 col1\" >4</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e2493_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "                        <td id=\"T_e2493_row13_col0\" class=\"data row13 col0\" >really</td>\n",
       "                        <td id=\"T_e2493_row13_col1\" class=\"data row13 col1\" >4</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e2493_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "                        <td id=\"T_e2493_row14_col0\" class=\"data row14 col0\" >make</td>\n",
       "                        <td id=\"T_e2493_row14_col1\" class=\"data row14 col1\" >4</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e2493_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "                        <td id=\"T_e2493_row15_col0\" class=\"data row15 col0\" >love</td>\n",
       "                        <td id=\"T_e2493_row15_col1\" class=\"data row15 col1\" >4</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e2493_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "                        <td id=\"T_e2493_row16_col0\" class=\"data row16 col0\" >right</td>\n",
       "                        <td id=\"T_e2493_row16_col1\" class=\"data row16 col1\" >3</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e2493_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "                        <td id=\"T_e2493_row17_col0\" class=\"data row17 col0\" >scamp</td>\n",
       "                        <td id=\"T_e2493_row17_col1\" class=\"data row17 col1\" >3</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e2493_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "                        <td id=\"T_e2493_row18_col0\" class=\"data row18 col0\" >got</td>\n",
       "                        <td id=\"T_e2493_row18_col1\" class=\"data row18 col1\" >3</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e2493_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "                        <td id=\"T_e2493_row19_col0\" class=\"data row19 col0\" >bad</td>\n",
       "                        <td id=\"T_e2493_row19_col1\" class=\"data row19 col1\" >3</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e2493_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "                        <td id=\"T_e2493_row20_col0\" class=\"data row20 col0\" >worth</td>\n",
       "                        <td id=\"T_e2493_row20_col1\" class=\"data row20 col1\" >3</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e2493_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "                        <td id=\"T_e2493_row21_col0\" class=\"data row21 col0\" >style</td>\n",
       "                        <td id=\"T_e2493_row21_col1\" class=\"data row21 col1\" >3</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e2493_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "                        <td id=\"T_e2493_row22_col0\" class=\"data row22 col0\" >interesting</td>\n",
       "                        <td id=\"T_e2493_row22_col1\" class=\"data row22 col1\" >3</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e2493_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "                        <td id=\"T_e2493_row23_col0\" class=\"data row23 col0\" >find</td>\n",
       "                        <td id=\"T_e2493_row23_col1\" class=\"data row23 col1\" >3</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e2493_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "                        <td id=\"T_e2493_row24_col0\" class=\"data row24 col0\" >look</td>\n",
       "                        <td id=\"T_e2493_row24_col1\" class=\"data row24 col1\" >3</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e2493_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "                        <td id=\"T_e2493_row25_col0\" class=\"data row25 col0\" >much</td>\n",
       "                        <td id=\"T_e2493_row25_col1\" class=\"data row25 col1\" >3</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e2493_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "                        <td id=\"T_e2493_row26_col0\" class=\"data row26 col0\" >life</td>\n",
       "                        <td id=\"T_e2493_row26_col1\" class=\"data row26 col1\" >3</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e2493_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "                        <td id=\"T_e2493_row27_col0\" class=\"data row27 col0\" >cinema</td>\n",
       "                        <td id=\"T_e2493_row27_col1\" class=\"data row27 col1\" >2</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e2493_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "                        <td id=\"T_e2493_row28_col0\" class=\"data row28 col0\" >little</td>\n",
       "                        <td id=\"T_e2493_row28_col1\" class=\"data row28 col1\" >2</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e2493_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "                        <td id=\"T_e2493_row29_col0\" class=\"data row29 col0\" >part</td>\n",
       "                        <td id=\"T_e2493_row29_col1\" class=\"data row29 col1\" >2</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1dc37f15be0>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating top 30 word by counts for positive reviews that were incorrectly predicted\n",
    "top_posmis = Counter([item for sublist in positive_mismatch['temp_list'] for item in sublist])\n",
    "temp_posmis = pd.DataFrame(top_posmis.most_common(30))\n",
    "temp_posmis.columns = ['Common_words','count']\n",
    "temp_posmis.style.background_gradient(cmap='OrRd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 30 words in the incorrect predictions dataset with actual polarity 0 are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe of all those instances with Polarity = 0 but were incorrectly predicted\n",
    "#We will check the word frequency in this dataframe\n",
    "negative_mismatch = df_mismatch[df_mismatch[\"Polarity\"]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_7dac6_row0_col1{\n",
       "            background-color:  #7f0000;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_7dac6_row1_col1{\n",
       "            background-color:  #f26d4b;\n",
       "            color:  #000000;\n",
       "        }#T_7dac6_row2_col1,#T_7dac6_row3_col1,#T_7dac6_row4_col1,#T_7dac6_row5_col1,#T_7dac6_row6_col1{\n",
       "            background-color:  #fdb27b;\n",
       "            color:  #000000;\n",
       "        }#T_7dac6_row7_col1,#T_7dac6_row8_col1,#T_7dac6_row9_col1,#T_7dac6_row10_col1,#T_7dac6_row11_col1,#T_7dac6_row12_col1,#T_7dac6_row13_col1,#T_7dac6_row14_col1,#T_7dac6_row15_col1,#T_7dac6_row16_col1,#T_7dac6_row17_col1,#T_7dac6_row18_col1,#T_7dac6_row19_col1,#T_7dac6_row20_col1{\n",
       "            background-color:  #fddcaf;\n",
       "            color:  #000000;\n",
       "        }#T_7dac6_row21_col1,#T_7dac6_row22_col1,#T_7dac6_row23_col1,#T_7dac6_row24_col1,#T_7dac6_row25_col1,#T_7dac6_row26_col1,#T_7dac6_row27_col1,#T_7dac6_row28_col1,#T_7dac6_row29_col1{\n",
       "            background-color:  #fff7ec;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_7dac6_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Common_words</th>        <th class=\"col_heading level0 col1\" >count</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_7dac6_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_7dac6_row0_col0\" class=\"data row0 col0\" >film</td>\n",
       "                        <td id=\"T_7dac6_row0_col1\" class=\"data row0 col1\" >6</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7dac6_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_7dac6_row1_col0\" class=\"data row1 col0\" >movie</td>\n",
       "                        <td id=\"T_7dac6_row1_col1\" class=\"data row1 col1\" >4</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7dac6_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_7dac6_row2_col0\" class=\"data row2 col0\" >acting</td>\n",
       "                        <td id=\"T_7dac6_row2_col1\" class=\"data row2 col1\" >3</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7dac6_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_7dac6_row3_col0\" class=\"data row3 col0\" >great</td>\n",
       "                        <td id=\"T_7dac6_row3_col1\" class=\"data row3 col1\" >3</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7dac6_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_7dac6_row4_col0\" class=\"data row4 col0\" >good</td>\n",
       "                        <td id=\"T_7dac6_row4_col1\" class=\"data row4 col1\" >3</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7dac6_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_7dac6_row5_col0\" class=\"data row5 col0\" >like</td>\n",
       "                        <td id=\"T_7dac6_row5_col1\" class=\"data row5 col1\" >3</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7dac6_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_7dac6_row6_col0\" class=\"data row6 col0\" >story</td>\n",
       "                        <td id=\"T_7dac6_row6_col1\" class=\"data row6 col1\" >3</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7dac6_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_7dac6_row7_col0\" class=\"data row7 col0\" >least</td>\n",
       "                        <td id=\"T_7dac6_row7_col1\" class=\"data row7 col1\" >2</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7dac6_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_7dac6_row8_col0\" class=\"data row8 col0\" >screen</td>\n",
       "                        <td id=\"T_7dac6_row8_col1\" class=\"data row8 col1\" >2</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7dac6_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_7dac6_row9_col0\" class=\"data row9 col0\" >two</td>\n",
       "                        <td id=\"T_7dac6_row9_col1\" class=\"data row9 col1\" >2</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7dac6_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "                        <td id=\"T_7dac6_row10_col0\" class=\"data row10 col0\" >could</td>\n",
       "                        <td id=\"T_7dac6_row10_col1\" class=\"data row10 col1\" >2</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7dac6_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "                        <td id=\"T_7dac6_row11_col0\" class=\"data row11 col0\" >wouldnt</td>\n",
       "                        <td id=\"T_7dac6_row11_col1\" class=\"data row11 col1\" >2</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7dac6_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "                        <td id=\"T_7dac6_row12_col0\" class=\"data row12 col0\" >one</td>\n",
       "                        <td id=\"T_7dac6_row12_col1\" class=\"data row12 col1\" >2</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7dac6_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "                        <td id=\"T_7dac6_row13_col0\" class=\"data row13 col0\" >whatever</td>\n",
       "                        <td id=\"T_7dac6_row13_col1\" class=\"data row13 col1\" >2</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7dac6_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "                        <td id=\"T_7dac6_row14_col0\" class=\"data row14 col0\" >beyond</td>\n",
       "                        <td id=\"T_7dac6_row14_col1\" class=\"data row14 col1\" >2</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7dac6_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "                        <td id=\"T_7dac6_row15_col0\" class=\"data row15 col0\" >see</td>\n",
       "                        <td id=\"T_7dac6_row15_col1\" class=\"data row15 col1\" >2</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7dac6_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "                        <td id=\"T_7dac6_row16_col0\" class=\"data row16 col0\" >actor</td>\n",
       "                        <td id=\"T_7dac6_row16_col1\" class=\"data row16 col1\" >2</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7dac6_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "                        <td id=\"T_7dac6_row17_col0\" class=\"data row17 col0\" >absolutely</td>\n",
       "                        <td id=\"T_7dac6_row17_col1\" class=\"data row17 col1\" >2</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7dac6_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "                        <td id=\"T_7dac6_row18_col0\" class=\"data row18 col0\" >abysmal</td>\n",
       "                        <td id=\"T_7dac6_row18_col1\" class=\"data row18 col1\" >2</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7dac6_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "                        <td id=\"T_7dac6_row19_col0\" class=\"data row19 col0\" >better</td>\n",
       "                        <td id=\"T_7dac6_row19_col1\" class=\"data row19 col1\" >2</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7dac6_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "                        <td id=\"T_7dac6_row20_col0\" class=\"data row20 col0\" >youre</td>\n",
       "                        <td id=\"T_7dac6_row20_col1\" class=\"data row20 col1\" >2</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7dac6_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "                        <td id=\"T_7dac6_row21_col0\" class=\"data row21 col0\" >couldnt</td>\n",
       "                        <td id=\"T_7dac6_row21_col1\" class=\"data row21 col1\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7dac6_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "                        <td id=\"T_7dac6_row22_col0\" class=\"data row22 col0\" >take</td>\n",
       "                        <td id=\"T_7dac6_row22_col1\" class=\"data row22 col1\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7dac6_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "                        <td id=\"T_7dac6_row23_col0\" class=\"data row23 col0\" >seriously</td>\n",
       "                        <td id=\"T_7dac6_row23_col1\" class=\"data row23 col1\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7dac6_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "                        <td id=\"T_7dac6_row24_col0\" class=\"data row24 col0\" >frightening</td>\n",
       "                        <td id=\"T_7dac6_row24_col1\" class=\"data row24 col1\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7dac6_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "                        <td id=\"T_7dac6_row25_col0\" class=\"data row25 col0\" >barely</td>\n",
       "                        <td id=\"T_7dac6_row25_col1\" class=\"data row25 col1\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7dac6_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "                        <td id=\"T_7dac6_row26_col0\" class=\"data row26 col0\" >comprehensible</td>\n",
       "                        <td id=\"T_7dac6_row26_col1\" class=\"data row26 col1\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7dac6_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "                        <td id=\"T_7dac6_row27_col0\" class=\"data row27 col0\" >considering</td>\n",
       "                        <td id=\"T_7dac6_row27_col1\" class=\"data row27 col1\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7dac6_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "                        <td id=\"T_7dac6_row28_col0\" class=\"data row28 col0\" >relation</td>\n",
       "                        <td id=\"T_7dac6_row28_col1\" class=\"data row28 col1\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7dac6_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "                        <td id=\"T_7dac6_row29_col0\" class=\"data row29 col0\" >taylor</td>\n",
       "                        <td id=\"T_7dac6_row29_col1\" class=\"data row29 col1\" >1</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1dc37f1dbe0>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating top 30 word by counts for negative reviews that were incorrectly predicted\n",
    "top_negmis = Counter([item for sublist in negative_mismatch['temp_list'] for item in sublist])\n",
    "temp_negmis = pd.DataFrame(top_negmis.most_common(30))\n",
    "temp_negmis.columns = ['Common_words','count']\n",
    "temp_negmis.style.background_gradient(cmap='OrRd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Analysis:\n",
    "\n",
    "Now when I look at the Word by Counts, I observe that the positive reviews with actual polarity == 1 that were incorrectly predicted, did not have any positive words like good, great, wonderful in the top 30 list by count. Instead, negative words like 'bad','never' were in the top 30 list. Similarly if we look at the top 30 words in the negative reviews with actual polarity == 0 but with incorrect prediction, we can see words like positive sentiments like 'good','great','better but no word with negative tone. Additionally, because the sentiment of the incorrectly predicted instances were also neutral, sentiments scores were also not helpful as a result machine learning algorithm only relied on the words for classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
